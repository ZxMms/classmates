# 学习笔记

## 1.函数式接口:使用Lamada表达式 传入参数与方法实现 替代接口 

```java
定义：接口都有@FunctionalInterface的注解。它注解在接口层面，且注解的接口要有且仅有一个抽象方法，Object的public方法除外。
作用：java不允许方法作为参数值传递 因此使用这个弥补 
因为方法（单独的方法）都是基于接口或者类定义的，不能独立存在。你可以传一个接口作为参数进去，在接口中定义方法，然后使用方法
():接口中抽象方法的参数列表,没有参数,就空着;有参数就写出参数,多个参数使用逗号分隔
->:传递的意思,把参数传递给方法体{}
{}:重写接口的抽象方法的方法体 
函数式接口：
    package com.sf;
    @FunctionalInterface
    public interface PersonService<T> {
        T execute(String a) throws Exception;//抽象方法
        default void sayHello(){
            System.out.println("heello");
        }
    }

调用方法：
    package com.sf;
    public class PersonServiceMain {
        public static void main(String[] args) {
            System.out.println(convertProductService());
        }
        public static String convertProductService() {
            return executeWrapper((a) -> {// 2
                // 校验参数
                String errorMsg = "";
                System.out.println(a);
                if ("".equals(errorMsg)) {
                    return "33";
                }
                return "44";
            });
        }

        private static String executeWrapper(PersonService<Object> func) {
            try {
                String a = "b";
                Object reuslt = func.execute(a);// 3
                return (String) reuslt;
            } catch (Exception e) {
                return "出现错误";
            }
        }
    }

结果：
b
33	
```



```java
例子：
接口：
/* 定一个厨子Cook接口，内含唯一的抽象方法makeFood */
public interface Cook {
    //定义无参数无返回值的方法makeFood
    public abstract void makeFood();
}
```

```java
实现：
/* 需求:
给定一个厨子Cook接口，内含唯一的抽象方法makeFood，且无参数、无返回值。
  使用Lambda的标准格式调用invokeCook方法，打印输出“吃饭啦！”字样 */
public class Demo01Cook {
    public static void main(String[] args) {
   //调用invokeCook方法,参数是Cook接口,传递Cook接口的匿名内部类对象
        invokeCook(new Cook() {
            @Override
            public void makeFood() {
                System.out.println("吃饭了");
            }
        });
        //使用Lambda表达式,简化匿名内部类的书写
        invokeCook(()->{
            System.out.println("吃饭了");
        });
        //优化省略Lambda
        invokeCook(()-> 
        System.out.println("吃饭了"));
    }
    //定义一个方法,参数传递Cook接口,方法内部调用Cook接口中的方法makeFood
    public static void invokeCook(Cook cook){
        cook.makeFood();
    }
}
```

​	函数式接口分类：

| 函数式接口       | 名字       | 函数描述符      | 作用                                        |
| ---------------- | ---------- | --------------- | ------------------------------------------- |
| Predicate<T>     | 断言式接口 | (T)  -> boolean | 通过参数值进行判断                          |
| Consumer<T>      | 消费者     | (T)  -> void    | 直接消费给的对象                            |
| Function< T, R > | 函数式接口 | (T)  -> R       | 将对象T转换为R                              |
| Supplier<T>      | 提供者     | ( )  -> T       | 提供者 不需要任何东西就可以提供返回一个对象 |



​	函数式接口主要应用场景：匿名内部类使用的地方 代码简化

消费者接口： 直接消费给的对象 不需要返回值 Consumer

```csharp
@Test
    public void test() {
        happy(10000,(m) -> System.out.println("大保健花了："+m));
    }
    public void happy(double  money,Consumer<Double> con) {
        con.accept(money);
    }
```

提供者接口：提供者 不需要任何东西就可以提供返回一个对象

```csharp
@Test
    public void test1() {
        List<Integer> numList = getNumList(10, ()->(int)(Math.random()*100 ));
        for (Integer integer : numList) {
            System.out.println(integer);
        }
    }
    
    //需求：产生指定个数的整数，并放入集合中
    public List<Integer> getNumList(int num,Supplier<Integer> sup){
            List<Integer> list = new ArrayList<>();
            for(int i=0;i<num;i++) {
                Integer n = sup.get();
                list.add(n);
                
            }
            return list;
    }
```

函数式接口：将一个对象转化为另一个对象

```kotlin
@Test
    public void  test2() {
        String trimStr=strHandler("\t\t  你好，world！   ",(str) -> str.trim());
        System.out.println(trimStr);
        String sumString=strHandler("Helloworld!",(str)->str.substring(2, 4));
        System.out.println(sumString);
    }
    //需求：用于处理字符串
    public  String strHandler(String str,Function<String,String> fun) {
        return fun.apply(str);
    }
    
```

断言式接口：判断 返回值为boolean

```dart
    @Test
    public void test3() {
        List<String> list=Arrays.asList("Hello","world","hi","o","123");
        List<String> filterStr = filterStr(list, (str)->str.length()>1);
        for (String string : filterStr) {
            System.out.println(string);
        }
    }
   
    //需求：将满足条件的字符串，放入集合中
        public List<String> filterStr(List<String> list, Predicate<String> pre){
            List<String> list2=new ArrayList<>();
            for (String str : list) {
                if(pre.test(str)){
                    list2.add(str);
                }
            }
            return list2;
        }
```

## 2.Iterator 和ListIterator

```java
	iterator()是一个接口，它是集合的迭代器。集合可以通过Iterator去遍历集合中的元素。其remove()方法是在迭代过程中唯一安全的删除方法。
	（1）Iterator只能单向移动。
	（2）Iterator.remove()是唯一安全的方式来在迭代过程中修改集合；如果在迭代过程中以任何其它的方式修改了基本集合将会产生未知的行为。而且每调用一次next()方法，remove()方法只能被调用一次，如果违反这个规则将抛出一个异常。
	原因：modAccount与expectedModCount 进行对比 不相等返回异常，其他remove()方法只修改结构，增加modAccount的值，不会修改expectedModCount的值，因此会出现异常，迭代器里面的每次remove()之后会修改两个值，使其相等。
        
        if (modCount != expectedModCount) {
            throw new ConcurrentModificationException();//抛出该异常
        }

	ListIterator是一个功能更加强大的, 它继承于Iterator接口,只能用于各种List类型的访问。可以通过调用listIterator()方法产生一个指向List开始处的ListIterator, 还可以调用listIterator(n)方法创建一个一开始就指向列表索引为n的元素处的ListIterator。
	由以上定义我们可以推出ListIterator可以:
    (1)双向移动（向前/向后遍历）.
    (2)产生相对于迭代器在列表中指向的当前位置的前一个和后一个元素的索引.
    (3)可以使用set()方法替换它访问过的最后一个元素.
    (4)可以使用add()方法在next()方法返回的元素之前或previous()方法返回的元素之后插入一个元素.
    
代码：
package com.sf;

import java.util.*;

public class TestListIteror {

	public static void main(String[] args) {
		ArrayList<String> a = new ArrayList<String>();
		a.add("aaa");
		a.add("bbb");
		a.add("ccc");
		System.out.println("Before iterate : " + a);
		ListIterator<String> it = a.listIterator();
		while (it.hasNext()) {
			System.out.println(it.next() + ", " + it.previousIndex() + ", "
					+ it.nextIndex());
		}
		while (it.hasPrevious()) {
			System.out.print(it.previous() + " ");
		}
		System.out.println();
		it = a.listIterator(1);// 调用listIterator(n)方法创建一个一开始就指向列表索引为n的元素处的ListIterator。
		while (it.hasNext()) {
			String t = it.next();
			System.out.println(t);
			if ("ccc".equals(t)) {
				it.set("nnn");
			} else {
				it.add("kkk");
			}
		}
		System.out.println("After iterate : " + a);
	}
}
结果：
Before iterate : [aaa, bbb, ccc]
aaa, 0, 1
bbb, 1, 2
ccc, 2, 3
ccc bbb aaa 
bbb
ccc
After iterate : [aaa, bbb, kkk, nnn]
	
```

## 	3.map

map的访问方式：

```java
1.通过Map.keySet()遍历key
     //第一种：普遍使用，二次取值
      System.out.println("通过Map.keySet遍历key和value：");
      for (String key : map.keySet()) {
       System.out.println("key= "+ key + " and value= " + map.get(key));
      }
2.通过Map.entrySet使用iterator遍历key和value： 拿取关系 使用迭代器
     System.out.println("通过Map.entrySet使用iterator遍历key和value：");
      Iterator<Map.Entry<String, String>> it = map.entrySet().iterator();
      while (it.hasNext()) {
       Map.Entry<String, String> entry = it.next();
       System.out.println("key= " + entry.getKey() + " and value= " + entry.getValue());
      }
3.通过Map.entrySet遍历key和value 常用  拿取关系
      System.out.println("通过Map.entrySet遍历key和value");
      for (Map.Entry<String, String> entry : map.entrySet()) {
       System.out.println("key= " + entry.getKey() + " and value= " + entry.getValue());
      }
4.遍历value1
     System.out.println("通过Map.values()遍历所有的value，但不能遍历key");
      for (String v : map.values()) {
       System.out.println("value= " + v);
      }

map.values():获取map的values值
map.keyset():获取map的key值
map.entrySet():获取对应的关系
```

## 4.排序方法

#### 	1.Java Comparable与Comparator区别（排序）

![img](https://img2018.cnblogs.com/blog/926003/201909/926003-20190926100130147-1742563712.png)

|       接口名称       |              Comparable               |               Comparator                |
| :------------------: | :-----------------------------------: | :-------------------------------------: |
|          包          |          java.lang（基础包）          |            java.util(工具包)            |
|         方法         |         compareTo(Object obj)         |      compare(Object o1, Object o2)      |
|       侵入式？       |       是，需要实体类实现该接口        |   不需要，创建比较器，直接传入比较器    |
|         意义         |       可比较的 需要侵入式实体类       |   比较算子  不需要侵入 重新创建的算子   |
| 是否可以使用集合排序 | 可以使用Collections.sort(Comparable); | 不可以使用Collections.sort(Comparator); |
|     是否可以使用     |   可以使用Arrays.sort（Comparable）   |    可以使用Arrays.sort（Comparator）    |
|       传入参数       |           一个  被比较对象            |           两个  互相比较对象            |
|         例子         |      compareTo(PeopleSolution o)      |      compare(People o1, People o2)      |

```java
例子：
	Comparable:
		1.类实现接口
			public class PeopleSolution implements Comparable<PeopleSolution>
		2.实现compareTo()方法：底层也是调用了Compare()方法
                
			@Override
            public int compareTo(PeopleSolution o) {
                // TODO Auto-generated method stub
                return this.getAge()-o.getAge();
            }

         public int compareTo(Integer anotherInteger) {
                return compare(this.value, anotherInteger.value);
            }
	Comparator:
		1.实体类没有任何侵入
         2.创建比较器的类 用该比较器实现Comparator接口
            public class PeopleSolution2 implements Comparator<People>
		3.实现Compare()方法
            @Override
            public int compare(People o1, People o2) {
                // TODO Auto-generated method stub
                return o1.getAge()-o2.getAge();
            }
	使用：
        logger.error("更新本地缓存失败: {}" ,AbnormalConditionProvider.class.getName(), e);
		Collections.sort(lsstp);  ：该参数为Comparable
		
		lsstp.forEach((s)->{System.out.println(s.toString());});
		
		Collections.sort(lsst, new PeopleSolution2());：第二个参数是比较器
            
   java 8中的排序方法：使用Comparator工具类进行排序 直接扩充属性进行排序
            
         使用Comparable()进行排序：
            
         lsstp.sort(Comparator.naturalOrder());：lsstp是一个实现comparable接口的实现类集合   按照Comparable中的CompareTo()方法属性进行排序 正序排列
          
         lsstp.sort(Comparator.reverseOrder());：逆序排列
            
         使用Comparator()进行排序：  
            //参数为比较器
         lsst.sort(Comparator.comparing(People::getId));：方便 简单 直接里面添加实体属性 升序排列
            
         lsst.sort(Comparator.comparingInt(Person::getAge).reversed());：降序排列
            
         comparingInt：使用函数式接口
         
         二次排序：
            
         // 先以价格（升序）、后再速度（升序）
        list.sort(Comparator.comparingInt(Computer::getPrice).thenComparingInt(Computer::getSpeed));
        // 先以速度（降序）、后再价格（升序）
        list.sort(Comparator.comparingInt(Computer::getSpeed).reversed().thenComparingInt(Computer::getPrice));
        // 先以价格（降序）、后再速度（降序）
        list.sort(Comparator.comparingInt(Computer::getPrice).thenComparingInt(Computer::getSpeed).reversed());

        
        问题：两个参数如何传进来？ 只传了一个GetId()  ？？ 解决
            
         :: 是一个函数式接口 对于ToIntFunction<? super T> keyExtractor 来说是转换接口  这个接口是转换接口  将其他类型转化为int类型 
            
         lsst.sort(Comparator.comparingInt(People::getId));

		所以调用的时候是People::getId   相当于将People 通过getID方法转化为int 
             
          @FunctionalInterface
          public interface ToIntFunction<T> {
            int applyAsInt(T value);
        }
            
        public static <T> Comparator<T> comparingInt(ToIntFunction<? super T> keyExtractor) {
            Objects.requireNonNull(keyExtractor);
            return (Comparator<T> & Serializable)
                (c1, c2) -> Integer.compare(keyExtractor.applyAsInt(c1), keyExtractor.applyAsInt(c2));
        }

		函数式接口：
        @FunctionalInterface
        public interface ToIntFunction<T> {
            int applyAsInt(T value);
        }		

		对于上述：只看方法名或者操作符即可 了解功能  Compare()方法被调用  因此是比较两个数据
            
        ToIntFunction<? super T> keyExtractor的接口实现难道是People::getId方法？
            
		但是使用函数是借口，利用lamada表达式可以查传入参数和方法实现  这块只有一个方法，单独方法不能作为参数

   		Java 8 中我们可以通过 "::" 关键字来访问类的构造方法，对象方法，静态方法。  这块也是一个函数式接口
            
      	Car::new是一个提供者 提供了一个car的对象
```



####     2.Java集合类排序方法

```
在使用Collection的sort排序的集合元素都必须是Comparable接口的实现类，该接口表示子类是可以比较的。因为实现接口必须重写抽象方法int compareTo(T t)方法。

Collections.sort()方法 

	Collections.sort(lsst，Compartor);：lsst存储的实体没有实现Compartor 接口  因此该方法会报错
		
	Collections.sort(lsstp);：lsstp存储的列表实现了Comparable接口
	
	lamada表达式：list.sort((o1, o2) -> o1.getId() - o2.getId());

```

## 5.java各个包的作用

| 包名        | 说明                                                         |
| :---------- | :----------------------------------------------------------- |
| java.lang   | 该包提供了Java编程的基础类，例如 Object、Math、String、StringBuffer、System、Thread等，不使用该包就很难编写Java代码了。 |
| java.util   | 该包提供了包含集合框架、遗留的集合类、事件模型、日期和时间实施、国际化和各种实用工具类（字符串标记生成器、随机数生成器和位数组）。 |
| java.io     | 该包通过文件系统、数据流和序列化提供系统的输入与输出。       |
| java.net    | 该包提供实现网络应用与开发的类。                             |
| java.sql    | 该包提供了使用Java语言访问并处理存储在数据源（通常是一个关系型数据库）中的数据API。 |
| java.awt    | 这两个包提供了GUI设计与开发的类。java.awt包提供了创建界面和绘制图形图像的所有类， |
| javax.swing | javax.swing包提供了一组“轻量级”的组件，尽量让这些组件在所有平台上的工作方式相同。 |
| java.text   | 提供了与自然语言无关的方式来处理文本、日期、数字和消息的类和接口。 |

## 6.泛型（10.18）

```
1.使用泛型而不是Object的原因：
比如CompareTo()方法，将之前的Object参数改成了泛型
    public interface Comparable<T> {	
        public int compareTo(T o);
    }
```

## 7.SPI 服务发现机制  服务提供者接口

SPI ，全称为 Service Provider Interface，是一种服务发现机制。它通过在ClassPath路径下的META-INF/services文件夹查找文件，自动加载文件里所定义的类。

```JAVA
//设置位置 源码中给定  约定大于配置
private static final String PREFIX = "META-INF/services/";

应用：JDBC
JDBC获取数据库连接的过程。在早期版本中，需要先设置数据库驱动的连接，再通过DriverManager.getConnection获取一个Connection。

String url = "jdbc:mysql:///consult?serverTimezone=UTC";
String user = "root";
String password = "root";
Class.forName("com.mysql.jdbc.Driver");
Connection connection = DriverManager.getConnection(url, user, password);

1、加载
    public class DriverManager {
        static {
            loadInitialDrivers();
            println("JDBC DriverManager initialized");
        }
    }	
具体过程还得看loadInitialDrivers，它在里面查找的是Driver接口的服务类，所以它的文件路径就是：META-INF/services/java.sql.Driver    其中内容 为com.mysql.cj.jdbc.Driver
    public class DriverManager {
        private static void loadInitialDrivers() {
            AccessController.doPrivileged(new PrivilegedAction<Void>() {
                public Void run() {
                    //很明显，它要加载Driver接口的服务类，Driver接口的包为:java.sql.Driver
                    //所以它要找的就是META-INF/services/java.sql.Driver文件 
                    /**
					* 查找接口实现类  还是在配置文件中
					*/
                    //ServiceLoader.load(SPIService.class);  加载该类的实现类
                    ServiceLoader<Driver> loadedDrivers = ServiceLoader.load(Driver.class);
                    
                    Iterator<Driver> driversIterator = loadedDrivers.iterator();
                    try{
                        //查到之后创建对象
                        while(driversIterator.hasNext()) {
                            driversIterator.next();
                        }
                    } catch(Throwable t) {
                        // Do nothing
                    }
                    return null;
                }
            });
        }
    }

2.创建实例

上一步已经找到了MySQL中的com.mysql.cj.jdbc.Driver全限定类名，当调用next方法时，就会创建这个类的实例。它就完成了一件事，向DriverManager注册自身的实例。
    public class Driver extends NonRegisteringDriver implements java.sql.Driver {
        static {
            try {
                //注册  将得到的Driver加入到列表中
                //调用DriverManager类的注册方法
                //往registeredDrivers集合中加入实例
                java.sql.DriverManager.registerDriver(new Driver());
            } catch (SQLException E) {
                throw new RuntimeException("Can't register driver!");
            }
        }
        public Driver() throws SQLException {
            // Required for Class.forName().newInstance()
        }
    }

3、创建Connection
在DriverManager.getConnection()方法就是创建连接的地方，它通过循环已注册的数据库驱动程序，调用其connect方法，获取连接并返回。
        private static Connection getConnection(
            String url, java.util.Properties info, Class<?> caller) throws SQLException {   
        //registeredDrivers中就包含com.mysql.cj.jdbc.Driver实例
        for(DriverInfo aDriver : registeredDrivers) {
            if(isDriverAllowed(aDriver.driver, callerCL)) {
                try {
                    //调用connect方法创建连接
                    Connection con = aDriver.driver.connect(url, info);
                    if (con != null) {
                        return (con);
                    }
                }catch (SQLException ex) {
                    if (reason == null) {
                        reason = ex;
                    }
                }
            } else {
                println("    skipping: " + aDriver.getClass().getName());
            }
        }
    }
4、再扩展
    既然我们知道JDBC是这样创建数据库连接的，我们能不能再扩展一下呢？如果我们自己也创建一个java.sql.Driver文件，自定义实现类MyDriver，那么，在获取连接的前后就可以动态修改一些信息。

还是先在项目ClassPath下创建文件，文件内容为自定义驱动类
我们的MyDriver实现类，继承自MySQL中的NonRegisteringDriver，还要实现java.sql.Driver接口。这样，在调用connect方法的时候，就会调用到此类，但实际创建的过程还靠MySQL完成。

    package com.viewscenes.netsupervisor.spi

    public class MyDriver extends NonRegisteringDriver implements Driver{
        static {
            try {
                java.sql.DriverManager.registerDriver(new MyDriver());
            } catch (SQLException E) {
                throw new RuntimeException("Can't register driver!");
            }
        }
        public MyDriver()throws SQLException {}

        public Connection connect(String url, Properties info) throws SQLException {
            System.out.println("准备创建数据库连接.url:"+url);
            System.out.println("JDBC配置信息："+info);
            info.setProperty("user", "root");
            Connection connection =  super.connect(url, info);
            System.out.println("数据库连接创建完成!"+connection.toString());
            return connection;
        }
    }
    --------------------输出结果---------------------
    准备创建数据库连接.url:jdbc:mysql:///consult?serverTimezone=UTC
    JDBC配置信息：{user=root, password=root}
    数据库连接创建完成!com.mysql.cj.jdbc.ConnectionImpl@7cf10a6f
```



## 9.ClassLoader与ContextClassLoader(类加载器与上下文加载器 10.18)

<img src="https://img-blog.csdn.net/20170922182004406?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI5MTI4MDM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img" style="zoom:33%;" />

这种机制就是，加载一个类的时候，会一直向上委托，如果BootStrapClassLoader加载不到，然后再依次往下加载，如果最后SystemClassLoader也加载不到，则会抛出classNotFoundException。

如果用“自定义clsloadr1”加载java.lang.String类，那么根据双亲委派最终bootstrap会加载此类，那么bootstrap类就叫做该类的“定义类加载器”，而包括bootstrap的所有得到该类class实例的类加载器都叫做“初始类加载器”。

一个类，由不同的类加载器实例加载的话，会在方法区产生两个不同的类，彼此不可见，并且在堆中生成不同Class实例。



Java 中的类加载器大致可以分成两类，一类是系统提供的，另外一类则是由 Java 应用开发人员编写的。系统提供的类加载器主要有下面三个：

- 引导类加载器（bootstrap class loader）：它用来加载 Java 的核心库，是用原生代码来实现的，并不继承自 `java.lang.ClassLoader` 。
- 扩展类加载器（extensions class loader）：它用来加载 Java 的扩展库。Java 虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载 Java 类。
- 系统类加载器（system class loader）：它根据 Java 应用的类路径（CLASSPATH）来加载 Java 类。一般来说，Java 应用的类都是由它来完成加载的。可以通过`ClassLoader.getSystemClassLoader()` 来获取它。

除了系统提供的类加载器以外，开发人员可以通过继承 `java.lang.ClassLoader` 类的方式实现自己的类加载器，以满足一些特殊的需求。

还用自定义加载器加载类：

```java
public void testClassIdentity() {
    String classDataRootPath = "C:\\workspace\\Classloader\\classData";
    FileSystemClassLoader fscl1 = new FileSystemClassLoader(classDataRootPath);
    FileSystemClassLoader fscl2 = new FileSystemClassLoader(classDataRootPath);
    String className = "com.example.Sample";
    try {
        Class<?> class1 = fscl1.loadClass(className);
        Object obj1 = class1.newInstance();
        Class<?> class2 = fscl2.loadClass(className);
        Object obj2 = class2.newInstance();
        Method setSampleMethod = class1.getMethod("setSample", java.lang.Object.class);
        setSampleMethod.invoke(obj1, obj2);
    } catch (Exception e) {
        e.printStackTrace();
    }
 }
```

了解了这一点之后，就可以理解代理模式的设计动机了。代理模式是为了保证 Java 核心库的类型安全。所有 Java 应用都至少需要引用 `java.lang.Object` 类，也就是说在运行的时候，`java.lang.Object` 这个类需要被加载到 Java 虚拟机中。如果这个加载过程由 Java 应用自己的类加载器来完成的话，很可能就存在多个版本的 `java.lang.Object` 类，而且这些类之间是不兼容的。通过代理模式，对于 Java 核心库的类的加载工作由引导类加载器来统一完成，保证了 Java 应用所使用的都是同一个版本的 Java 核心库的类，是互相兼容的。

不同的类加载器为相同名称的类创建了额外的名称空间。相同名称的类可以并存在 Java 虚拟机中，只需要用不同的类加载器来加载它们即可。不同类加载器加载的类之间是不兼容的，这就相当于在 Java 虚拟机内部创建了一个个相互隔离的 Java 类空间。这种技术在许多框架中都被用到，后面会详细介绍。

### 委托代理

类加载器在尝试自己去查找某个类的字节代码并定义它时，会先代理给其父类加载器，由父类加载器先去尝试加载这个类，依次类推。在介绍代理模式背后的动机之前，首先需要说明一下 Java 虚拟机是如何判定两个 Java 类是相同的。Java 虚拟机不仅要看类的全名是否相同，还要看加载此类的类加载器是否一样。只有两者都相同的情况，才认为两个类是相同的。即便是同样的字节代码，被不同的类加载器加载之后所得到的类，也是不同的。比如一个 Java 类`com.example.Sample` ，编译之后生成了字节代码文件 `Sample.class` 。两个不同的类加载器`ClassLoaderA` 和 `ClassLoaderB` 分别读取了这个 `Sample.class` 文件，并定义出两个`java.lang.Class` 类的实例来表示这个类。这两个实例是不相同的。对于 Java 虚拟机来说，它们是不同的类。试图对这两个类的对象进行相互赋值，会抛出运行时异常 `ClassCastException` 。

前面介绍类加载器的代理模式的时候，提到过类加载器会首先代理给其它类加载器来尝试加载某个类。这就意味着真正完成类的加载工作的类加载器和启动这个加载过程的类加载器，有可能不是同一个。真正完成类的加载工作是通过调用 `defineClass` 来实现的；而启动类的加载过程是通过调用 `loadClass` 来实现的。前者称为一个类的定义加载器（defining loader），后者称为初始加载器（initiating loader）。在 Java 虚拟机判断两个类是否相同的时候，使用的是类的定义加载器。也就是说，哪个类加载器启动类的加载过程并不重要，重要的是最终定义这个类的加载器。两种类加载器的关联之处在于：一个类的定义加载器是它引用的其它类的初始加载器。如类`com.example.Outer` 引用了类 `com.example.Inner` ，则由类 `com.example.Outer` 的定义加载器负责启动类 `com.example.Inner` 的加载过程。

方法 `loadClass()` 抛出的是 `java.lang.ClassNotFoundException` 异常；方法 `defineClass()` 抛出的是 `java.lang.NoClassDefFoundError` 异常。类加载器在成功加载某个类之后，会把得到的 `java.lang.Class` 类的实例缓存起来。下次再请求加载该类的时候，类加载器会直接使用缓存的类的实例，而不会尝试再次加载。也就是说，对于一个类加载器实例来说，相同全名的类只加载一次，即 `loadClass` 方法不会被重复调用。

### ContextClassLOader(上下文类加载器)

线程上下文类加载器（context class loader）是从 JDK 1.2 开始引入的。类 `java.lang.Thread` 中的方法 `getContextClassLoader()` 和 `setContextClassLoader(ClassLoader cl)` 用来获取和设置线程的上下文类加载器。如果没有通过 `setContextClassLoader(ClassLoader cl)` 方法进行设置的话，线程将继承其父线程的上下文类加载器。Java 应用运行的初始线程的上下文类加载器是系统类加载器。在线程中运行的代码可以通过此类加载器来加载类和资源。

前面提到的类加载器的代理模式并不能解决 Java 应用开发中会遇到的类加载器的全部问题。Java 提供了很多服务提供者接口（Service Provider Interface，SPI），允许第三方为这些接口提供实现。常见的 SPI 有 JDBC、JCE、JNDI、JAXP 和 JBI 等。这些 SPI 的接口由 Java 核心库来提供，如 JAXP 的 SPI 接口定义包含在 `javax.xml.parsers` 包中。这些 SPI 的实现代码很可能是作为 Java 应用所依赖的 jar 包被包含进来，可以通过类路径（CLASSPATH）来找到，如实现了 JAXP SPI 的 [Apache Xerces](http://xerces.apache.org/) 所包含的 jar 包。SPI 接口中的代码经常需要加载具体的实现类。如 JAXP 中的 `javax.xml.parsers.DocumentBuilderFactory` 类中的 `newInstance()` 方法用来生成一个新的 `DocumentBuilderFactory` 的实例。这里的实例的真正的类是继承自`javax.xml.parsers.DocumentBuilderFactory` ，由 SPI 的实现所提供的。如在 Apache Xerces 中，实现的类是 `org.apache.xerces.jaxp.DocumentBuilderFactoryImpl` 。而问题在于，SPI 的接口是 Java 核心库的一部分，是由引导类加载器来加载的；SPI 实现的 Java 类一般是由系统类加载器来加载的。引导类加载器是无法找到 SPI 的实现类的，因为它只加载 Java 的核心库。它也不能代理给系统类加载器，因为它是系统类加载器的祖先类加载器。也就是说，类加载器的代理模式无法解决这个问题。

线程上下文类加载器正好解决了这个问题。如果不做任何的设置，Java 应用的线程的上下文类加载器默认就是系统上下文类加载器。在 SPI 接口的代码中使用线程上下文类加载器，就可以成功的加载到 SPI 实现的类。线程上下文类加载器在很多 SPI 的实现中都会用到。

### classForName

```
`Class.forName` 是一个静态方法，同样可以用来加载类。该方法有两种形式：`Class.forName(String name, boolean initialize, ClassLoader loader)` 和`Class.forName(String className)` 。第一种形式的参数 `name` 表示的是类的全名； `initialize`表示是否初始化类；`loader` 表示加载时使用的类加载器。第二种形式则相当于设置了参数`initialize` 的值为 `true`，`loader` 的值为当前类的类加载器。`Class.forName` 的一个很常见的用法是在加载数据库驱动的时候。如`Class.forName("org.apache.derby.jdbc.EmbeddedDriver").newInstance()` 用来加载 Apache Derby 数据库的驱动。
```

### 上下文

```
上下文可以理解为工程的环境，那么这个环境信包含了一些配置信息 ，就比如：spring上下文，代表的是spring配置的环境信息，我们spring配置文件是applicationcontext.xml，application.xml就是spring的上下文，也就是读取到这个上下文，就可以获得spring的配置数据，总的来说上下文对应配置文件，因为配置文件是配置环境的，获得上下文，就是获得了环境的信息，也就是获得了你配置文件的信息，所以经常获取spring的上下文，就是直接读取spring的配置文件，因为这个配置文件配置的环境信息呀
```

## 10.设计模式



## 11.lamada表达式

```
基本Lamada表达式：

    (参数)->{方法}

    ():接口中抽象方法的参数列表,没有参数,就空着;有参数就写出参数,多个参数使用逗号分隔
    ->:传递的意思,把参数传递给方法体{}
    {}:重写接口的抽象方法的方法体 
    
Stream()流的使用：  主要对集合进行操作 添加 修改 过滤 求和 之类的

1.替代匿名内部类
	匿名内部类：
	    @Test
        public void oldRunable() {
            new Thread(new Runnable() {
                @Override
                public void run() {
                    System.out.println("The old runable now is using!");
                }
            }).start();
        }
    使用lamada表达式：
    	@Test
        public void runable() {
            new Thread(() -> System.out.println("It's a lambda function!")).start();
        }
        
 2.使用lambda表达式对集合进行迭代  forEach()
 
    	@Test
        public void iterTest() {
            List<String> languages = Arrays.asList("java","scala","python");
            //before java8
            for(String each:languages) {
                System.out.println(each);
            }
            //after java8
            languages.forEach(x -> System.out.println(x));
            languages.forEach(System.out::println);
        }
        
3.  用lambda表达式实现对象转化 map()
	    @Test
        public void mapTest() {
            List<Double> cost = Arrays.asList(10.0, 20.0,30.0);
            cost.stream().map(x -> x + x*0.05).forEach(System.out::println);
        }
        
4.用lambda表达式实现map与reduce  reduce() 求和

        @Test
        public void mapReduceTest() {
            List<Double> cost = Arrays.asList(10.0, 20.0,30.0);
            double allCost = cost.stream().map(x -> x+x*0.05).reduce((sum,x) -> sum + x).get();
            System.out.println(allCost);
        }
5.filter过滤操作  collect(Collectors.toList())将其搞成一个list组合

        @Test
        public void filterTest() {
            List<Double> cost = Arrays.asList(10.0, 20.0,30.0,40.0);
            List<Double> filteredCost = cost.stream().filter(x -> x > 25.0).collect(Collectors.toList());
            filteredCost.forEach(x -> System.out.println(x));
            
            //简化版
            cost.stream().filter(x -> x > 25.0).collect(Collectors.toList()).forEach(System.out::println);
        }
6.与函数式接口Predicate配合

        public static void filterTest(List<String> languages, Predicate<String> condition) {
            languages.stream().filter(x -> condition.test(x)).forEach(x -> System.out.println(x + " "));
        }

        public static void main(String[] args) {
            List<String> languages = Arrays.asList("Java","Python","scala","Shell","R");
            System.out.println("Language starts with J: ");
            filterTest(languages,x -> x.startsWith("J"));
            System.out.println("\nLanguage ends with a: ");
            filterTest(languages,x -> x.endsWith("a"));
            System.out.println("\nAll languages: ");
            filterTest(languages,x -> true);
            System.out.println("\nNo languages: ");
            filterTest(languages,x -> false);
            System.out.println("\nLanguage length bigger three: ");
            filterTest(languages,x -> x.length() > 4);
        }
7.排序代码
List<Person> collect=list.stream().sorted(Comparator.comparing(Person::getSalary)).collect(Collectors.toList());

```

## 13.kafka https://www.cnblogs.com/qingyunzong/p/9004509.html

```
一个消息系统负责将数据从一个应用传递到另外一个应用，应用只需关注于数据，无需关注数据在两个或多个应用间是如何传递的。分布式消息传递基于可靠的消息队列，在客户端应用和消息系统之间异步传递消息。有两种主要的消息传递模式：点对点传递模式、发布-订阅模式。大部分的消息系统选用发布-订阅模式。Kafka就是一种发布-订阅模式。

kafka中每个分区都是有序且顺序不可变的记录集，并且不断地追加到结构化的commit log文件。分区中的每一个记录都会分配一个id号来表示顺序，我们称之为offset，offset用来唯一的标识分区中每一条记录.
```

### 点对点模式

在点对点消息系统中，消息持久化到一个队列中。此时，将有一个或多个消费者消费队列中的数据。但是一条消息只能被消费一次。当一个消费者消费了队列中的某条数据之后，该条数据则从消息队列中删除。该模式即使有多个消费者同时消费数据，也能保证数据处理的顺序。这种架构描述示意图如下：

生产者发送一条消息到queue，只有一个消费者能收到。

![img](https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507190326476-771565746.png)

### 发布订阅

在发布-订阅消息系统中，消息被持久化到一个topic中。与点对点消息系统不同的是，消费者可以订阅一个或多个topic，消费者可以消费该topic中所有的数据，同一条数据可以被多个消费者消费，数据被消费后不会立马删除。在发布-订阅消息系统中，消息的生产者称为发布者，消费者称为订阅者。该模式的示例图如下：

![img](https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507190443404-1266011458.png)

**发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息**。

Kafka保证一个Partition内的消息的有序性。

在深入理解Kafka之前，先介绍一下Kafka中的术语。下图展示了Kafka的相关术语以及之间的关系：

![img](https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507190731172-1317551019.png)

上图中一个topic配置了3个partition。Partition1有两个offset：0和1。Partition2有4个offset。Partition3有1个offset。副本的id和副本所在的机器的id恰好相同。

如果一个topic的副本数为3，那么Kafka将在集群中为每个partition创建3个相同的副本。集群中的每个broker存储一个或多个partition。多个producer和consumer可同时生产和消费数据。

### Broker

Kafka 集群包含一个或多个服务器，服务器节点称为broker。

broker存储topic的数据。如果某topic有N个partition，集群有N个broker，那么每个broker存储该topic的一个partition。

如果某topic有N个partition，集群有(N+M)个broker，那么其中有N个broker存储该topic的一个partition，剩下的M个broker不存储该topic的partition数据。

如果某topic有N个partition，集群中broker数目少于N个，那么一个broker存储该topic的一个或多个partition。在实际生产环境中，尽量避免这种情况的发生，这种情况容易导致Kafka集群数据不均衡。

### Topic

每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）

类似于数据库的表名

### Partition

topic中的数据分割为一个或多个partition。每个topic至少有一个partition。每个partition中的数据使用多个segment文件存储。partition中的数据是有序的，不同partition间的数据丢失了数据的顺序。如果topic有多个partition，消费数据时就不能保证数据的顺序。在需要严格保证消息的消费顺序的场景下，需要将partition数目设为1。

### 消费者

消费者使用一个 *消费组* 名称来进行标识，发布到topic中的每条记录被分配给订阅消费组中的一个消费者实例.消费者实例可以分布在多个进程中或者多个机器上。

如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例.

如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程.

![img](https://kafka.apachecn.org/10/images/consumer-groups.png)

如图，这个 Kafka 集群有两台 server 的，四个分区(p0-p3)和两个消费者组。消费组A有两个消费者，消费组B有四个消费者。

通常情况下，每个 topic 都会有一些消费组，一个消费组对应一个"逻辑订阅者"。一个消费组由许多消费者实例组成，便于扩展和容错。这就是发布和订阅的概念，只不过订阅者是一组消费者而不是单个的进程。

在Kafka中实现消费的方式是将日志中的分区划分到每一个消费者实例上，以便在任何时间，每个实例都是分区唯一的消费者。维护消费组中的消费关系由Kafka协议动态处理。如果新的实例加入组，他们将从组中其他成员处接管一些 partition 分区;如果一个实例消失，拥有的分区将被分发到剩余的实例。

Kafka 只保证分区内的记录是有序的，而不保证主题中不同分区的顺序。每个 partition 分区按照key值排序足以满足大多数应用程序的需求。但如果你需要总记录在所有记录的上面，可使用仅有一个分区的主题来实现，这意味着每个消费者组只有一个消费者进程。



上面说到数据会写入到不同的分区，那kafka为什么要做分区呢？相信大家应该也能猜到，分区的主要目的是：
　　**1、	方便扩展**。因为一个topic可以有多个partition，所以我们可以通过扩展机器去轻松的应对日益增长的数据量。
　　**2、	提高并发**。以partition为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率。

　　熟悉负载均衡的朋友应该知道，当我们向某个服务器发送请求的时候，服务端可能会对请求做一个负载，将流量分发到不同的服务器，那在kafka中，如果某个topic有多个partition，producer又怎么知道该将数据发往哪个partition呢？kafka中有几个原则：
　　1、	partition在写入的时候可以指定需要写入的partition，如果有指定，则写入对应的partition。
　　2、	如果没有指定partition，但是设置了数据的key，则会根据key的值hash出一个partition。
　　3、	如果既没指定partition，又没有设置key，则会轮询选出一个partition。

　　![img](https://www.17coding.info/cdn/WeChat%20Screenshot_20190325215252.png)

保证消息不丢失是一个消息队列中间件的基本保证，那producer在向kafka写入消息的时候，怎么保证消息不丢失呢？其实上面的写入流程图中有描述出来，那就是通过ACK应答机制！在生产者向队列写入数据的时候可以设置参数来确定是否确认kafka接收到数据，这个参数可设置的值为**0**、**1**、**all**。
　　0代表producer往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。
　　1代表producer往集群发送数据只要leader应答就可以发送下一条，只确保leader发送成功。
　　all代表producer往集群发送数据需要所有的follower都完成从leader的同步才会发送下一条，确保leader发送成功和所有的副本都完成备份。安全性最高，但是效率最低。

　　最后要注意的是，如果往不存在的topic写数据，能不能写入成功呢？kafka会自动创建topic，分区和副本的数量根据默认配置都是1。



Producer将数据写入kafka后，集群就需要对数据进行保存了！kafka将数据保存在磁盘，可能在我们的一般的认知里，写入磁盘是比较耗时的操作，不适合这种高并发的组件。Kafka初始会单独开辟一块磁盘空间，顺序写入数据（效率比随机写入高）。

**Partition 结构**
　　前面说过了每个topic都可以分为一个或多个partition，如果你觉得topic比较抽象，那partition就是比较具体的东西了！Partition在服务器上的表现形式就是一个一个的文件夹，每个partition的文件夹下面会有多组segment文件，每组segment文件又包含.index文件、.log文件、.timeindex文件（早期版本中没有）三个文件， log文件就实际是存储message的地方，而index和timeindex文件为索引文件，用于检索消息。

![img](https://www.17coding.info/cdn/WeChat%20Screenshot_20190325215337.png)

如上图，这个partition有三组segment文件，每个log文件的大小是一样的，但是存储的message数量是不一定相等的（每条的message大小不一致）。文件的命名是以该segment最小offset来命名的，如000.index存储offset为0~368795的消息，kafka就是利用**分段+索引**的方式来解决查找效率的问题。

**Message结构**
上面说到log文件就实际是存储message的地方，我们在producer往kafka写入的也是一条一条的message，那存储在log中的message是什么样子的呢？消息主要包含消息体、消息大小、offset、压缩类型……等等！我们重点需要知道的是下面三个：
　　1、	offset：offset是一个占8byte的有序id号，它可以唯一确定每条消息在parition内的位置！
　　2、	消息大小：消息大小占用4byte，用于描述消息的大小。
　　3、	消息体：消息体存放的是实际的消息数据（被压缩过），占用的空间根据具体的消息而不一样。

**存储策略**
　　无论消息是否被消费，kafka都会保存所有的消息。那对于旧数据有什么删除策略呢？
　　1、	基于时间，默认配置是168小时（7天）。
　　2、	基于大小，默认配置是1073741824。
　　需要注意的是，kafka读取特定消息的时间复杂度是O(1)，所以这里删除过期的文件并不会提高kafka的性能！

### 消费数据

　　消息存储在log文件后，消费者就可以进行消费了。与生产消息相同的是，消费者在拉取消息的时候也是**找leader**去拉取。

　　多个消费者可以组成一个消费者组（consumer group），每个消费者组都有一个组id！同一个消费组者的消费者可以消费同一topic下不同分区的数据，但是不会组内多个消费者消费同一分区的数据！！！是不是有点绕。我们看下图：

![img](https://www.17coding.info/cdn/WeChat%20Screenshot_20190325215326.png)

查找消息的时候是怎么利用segment+offset配合查找的呢？假如现在需要查找一个offset为368801的message是什么样的过程呢？我们先看看下面的图：

![img](https://www.17coding.info/cdn/WeChat%20Screenshot_20190325215338.png)

1、	先找到offset的368801message所在的segment文件（利用二分法查找），这里找到的就是在第二个segment文件。
　　2、	打开找到的segment中的.index文件（也就是368796.index文件，该文件起始偏移量为368796+1，我们要查找的offset为368801的message在该index内的偏移量为368796+5=368801，所以这里要查找的**相对offset**为5）。由于该文件采用的是稀疏索引的方式存储着相对offset及对应message物理偏移量的关系，所以直接找相对offset为5的索引找不到，这里同样利用二分法查找相对offset小于或者等于指定的相对offset的索引条目中最大的那个相对offset，所以找到的是相对offset为4的这个索引。
　　3、	根据找到的相对offset为4的索引确定message存储的物理偏移位置为256。打开数据文件，从位置为256的那个地方开始顺序扫描直到找到offset为368801的那条Message。

　　这套机制是建立在offset为有序的基础上，利用**segment**+**有序offset**+**稀疏索引**+**二分查找**+**顺序查找**等多种手段来高效的查找数据！至此，消费者就能拿到需要处理的数据进行处理了。那每个消费者又是怎么记录自己消费的位置呢？在早期的版本中，消费者将消费到的offset维护zookeeper中，consumer每间隔一段时间上报一次，这里容易导致重复消费，且性能不好！在新的版本中消费者消费到的offset已经直接维护在kafk集群的__consumer_offsets这个topic中！



    生产者：
    1.设置属性值 地址 topic 
      Properties p = new Properties();
      p.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");//kafka地址，多个地址用逗号分割
      p.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
      p.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
    2.创建生产者对象
    KafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(p);
    3.发送消息
    	 try {
                while (true) {
                    String msg = "Hello," + new Random().nextInt(100);
                    ProducerRecord<String, String> record = new ProducerRecord<String, String>(topic, msg);
                    kafkaProducer.send(record);
                    System.out.println("消息发送成功:" + msg);
                    Thread.sleep(500);
                }
            } finally {
                kafkaProducer.close();
            }
         
    消费者：
    1.设置属性值
     Properties p = new Properties();
     p.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
     p.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
     p.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
     p.put(ConsumerConfig.GROUP_ID_CONFIG, "duanjt_test");
    2.创建消费者
     KafkaConsumer<String, String> kafkaConsumer = new KafkaConsumer<String, String>(p);
    3.订阅
     kafkaConsumer.subscribe(Collections.singletonList(Producer.topic));// 订阅消息
    4接收消息
     	 while (true) {
                ConsumerRecords<String, String> records = kafkaConsumer.poll(100);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.println(String.format("topic:%s,offset:%d,消息:%s", //
                            record.topic(), record.offset(), record.value()));
                }
            }
    
    
    


## 14.redis

```

```

## 15.Lombok常用注释：

```java
1. @Getter/@Setter
自动产生 getter/setter

2. @ToString
自动重写 toString() 方法，打印出所有变量

3. @EqualsAndHashCode
自动生成 equals(Object other) 和 hashcode() 方法，包括所有非静态变量和非 transient 的变量

4. @NoArgsConstructor, @AllArgsConstructor, @RequiredArgsConstructor
这三个很像，都是在自动生成该类的构造器，差别只在生成的构造器的参数不一样而已
	@NoArgsConstructor : 生成一个没有参数的构造器
    @AllArgsConstructor : 生成一个包含所有参数的构造器
    @RequiredArgsConstructor : 生成一个包含 "特定参数" 的构造器，特定参数指的是那些有加上 final 修饰词的变量们

5. @Data整合包，只要加了 @Data 这个注解，等于同时加了以下注解
    @Getter/@Setter
    @ToString
    @EqualsAndHashCode
    @RequiredArgsConstructor
6. @Value也是整合包，但是他会把所有的变量都设成 final 的，其他的就跟 @Data 一样，等于同时加了以下注解

    @Getter (注意没有setter)
    @ToString
    @EqualsAndHashCode
    @RequiredArgsConstructor

7. @Builder自动生成流式 set 值写法，从此之后再也不用写一堆 setter 了 所以通常是 @Data 和 @Builder 会一起用在同个类上，既方便我们流式写代码，也方便框架做事
        
        Perpon p=Perpon.builder().id(1).name("44").age(2).className("维恩").build();
   
8.@Slf4j
自动生成该类的 log 静态常量，要打日志就可以直接打，不用再手动 new log 静态常量了
```

## 16.equals与hashcode问题

```
这两个方法属于Object类：
    public boolean equals(Object obj)
    public int hashCode()
    
     1.equal()相等的两个对象他们的hashCode()肯定相等，也就是用equal()对比是绝对可靠的。
     2.hashCode()相等的两个对象他们的equal()不一定相等，有可能存在hash冲突，即两个对象不相等，hash值相同，也就是hashCode()不是绝对可靠的。
     
hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode() 定义在JDK的Object.java中，这就意味着Java中的任何类都包含有hashCode() 函数。

equals它的作用也是判断两个对象是否相等，如果对象重写了equals()方法，比较两个对象的内容是否相等；如果没有重写，比较两个对象的地址是否相同，价于“==”。同样的，equals()定义在JDK的Object.java中，这就意味着Java中的任何类都包含有equals()函数。

如果两个对象相等，那么它们的hashCode()值一定相同。这里的相等是指，通过equals()比较两个对象时返回true。

如果两个对象hashCode()相等，它们并不一定相等。因为在散列表中，hashCode()相等，即两个键值对的哈希值相等。然而哈希值相等，并不一定能得出键值对相等，此时就出现所谓的哈希冲突场景。

两个对象相等，hashcode一定相等
两个对象不等，hashcode不一定不等
hashcode相等，两个对象不一定相等
hashcode不等，两个对象一定不等

原因：
重写equals方法比较值，不重写比较地址，hashcode重写与属性值有关，不重写返回对象的地址值。
1.对于两个对象来说，如果重新equals方法，当两个对象相等时，其equals方法相等，但是由于是两个对象，因此其位置不一样，所以hashcode不一样，因此要重写hashcode方法，使得两个对象相等 equals方法相等且hashcode相等。
2.对于集合类来说 一定要重写这两个方法 否则插入进去的相等对象可能会都插入进去，毕竟hashcode不同，认为是两个对象

```

## 17.Apache CXF（java访问wsdl接口）



## 18.Mapper文件

```xml
1.resultType：

    resultType是sql映射文件中定义返回值类型，返回值有基本类型，对象类型，List类型，Map类型等。现总结一下再解释

    总结：
    resultType:

    1、基本类型  ：resultType=基本类型

    2、List类型：   resultType=List中元素的类型

    3、Map类型    单条记录：resultType =map

                 多条记录：resultType =Map中value的类型
 2.resultMap:
 <resultMap id="BaseResultMap" type="dd.springboot.demo.models.TestZx">
    <id column="id" jdbcType="INTEGER" property="id" />
    <result column="name" jdbcType="VARCHAR" property="name" />
    <result column="sex" jdbcType="VARCHAR" property="sex" />
    <result column="birthday" jdbcType="TIMESTAMP" property="birthday" />
  </resultMap>
  其中：
 	column：从数据库查询出来的属性名
 	property：实体类具有的属性名
 	
 resultType：当使用resultType做SQL语句返回结果类型处理时，对于SQL语句查询出的字段在相应的pojo中必须有和它相同的字段对应，而resultType中的内容就是pojo在本项目中的位置。

因此对于单表查询的话用resultType是最合适的。但是，如果在写pojo时，不想用数据库表中定义的字段名称，也是可以使用resultMap进行处理对应的。多表连接查询时，若是一对一的连接查询，那么需要新建一个pojo，pojo中包括两个表中需要查询出的所有的字段，这个地方的处理方式通常为创建一个继承一个表字段的pojo，再在里面添加另外一个表内需要查询出的字段即可。若是一对多查询时，若是使用内连接查询，则很可能出现查询出的字段有重复。使用双重for循环嵌套处理即可。

resultMap：当使用resultMap做SQL语句返回结果类型处理时，通常需要在mapper.xml中定义resultMap进行pojo和相应表字段的对应。
 
resultMap对于一对一表连接的处理方式通常为在主表的pojo中添加嵌套另一个表的pojo，然后在mapper.xml中采用association节点元素进行对另一个表的连接处理。例如：
 一对一的关系：
    <!-- 订单查询关联用户的resultMap  
        将整个查询的结果映射到cn.itcast.mybatis.po.Orders中  
         -->  
        <resultMap type="cn.itcast.mybatis.po.Orders" id="OrdersUserResultMap">  
            <!-- 配置映射的订单信息 -->  
            <!-- id：指定查询列中的唯 一标识，订单信息的中的唯 一标识，如果有多个列组成唯一标识，配置多个id  
                column：订单信息的唯 一标识 列  
                property：订单信息的唯 一标识 列所映射到Orders中哪个属性  
              -->  
            <id column="id" property="id"/>  
            <result column="user_id" property="userId"/>  
            <result column="number" property="number"/>  
            <result column="createtime" property="createtime"/>  
            <result column="note" property=note/>  

            <!-- 配置映射的关联的用户信息 -->  
            <!-- association：用于映射关联查询单个对象的信息  
            property：要将关联查询的用户信息映射到Orders中哪个属性  
             -->  
            <association property="user"  javaType="cn.itcast.mybatis.po.User">  
                <!-- id：关联查询用户的唯 一标识  
                column：指定唯 一标识用户信息的列  
                javaType：映射到user的哪个属性  
                 -->  
                <id column="user_id" property="id"/>  
                <result column="username" property="username"/>  
                <result column="sex" property="sex"/>  
                <result column="address" property="address"/>  

            </association>  
        </resultMap>  	
若是一对多的表连接方式，比如订单表和订单明细表即为一对多连接，若是不对sql语句进行处理，由于一个订单对应多条订单明细，因此查询出的结果对于订单表数据来说将会出现重复，例如：

resultMap的处理方式为在订单表数据的pojo中添加一个list,list中为订单明细表的属性，在mapper.xml中采用如下的处理方式：

 一对多的关系：
        <!-- 订单及订单明细的resultMap  
        使用extends继承，不用在中配置订单信息和用户信息的映射  
         -->  
        <resultMap type="cn.itcast.mybatis.po.Orders" id="OrdersAndOrderDetailResultMap" extends="OrdersUserResultMap">  
            <!-- 订单信息 -->  
            <!-- 用户信息 -->  
            <!-- 使用extends继承，不用在中配置订单信息和用户信息的映射 -->  
            <!-- 订单明细信息  
            一个订单关联查询出了多条明细，要使用collection进行映射  
            collection：对关联查询到多条记录映射到集合对象中  
            property：将关联查询到多条记录映射到cn.itcast.mybatis.po.Orders哪个属性  
            ofType：指定映射到list集合属性中pojo的类型  
             -->  
             <collection property="orderdetails" ofType="cn.itcast.mybatis.po.Orderdetail">  
                <!-- id：订单明细唯 一标识  
                property:要将订单明细的唯 一标识 映射到cn.itcast.mybatis.po.Orderdetail的哪个属性  
                  -->  
                <id column="orderdetail_id" property="id"/>  
                <result column="items_id" property="itemsId"/>  
                <result column="items_num" property="itemsNum"/>  
                <result column="orders_id" property="ordersId"/>  
             </collection>  
        </resultMap>  

 3.#与$的区别：
 
     1 #是将传入的值当做字符串的形式，eg:select id,name,age from student where id =#{id},当前端把id值1，传入到后台的时候，就相当于 select id,name,age from student where id ='1'.

     2 $是将传入的数据直接显示生成sql语句，eg:select id,name,age from student where id =${id},当前端把id值1，传入到后台的时候，就相当于 select id,name,age from student where id = 1. 

     3 使用#可以很大程度上防止sql注入。(语句的拼接) and service_template_name like CONCAT('%', #{serviceTemplateName,jdbcType=VARCHAR},'%') 

     4 但是如果使用在order by 中就需要使用 $.

     5 在大多数情况下还是经常使用#，但在不同情况下必须使用$. 

    我觉得#与{}穿入值，sql解析时，参数是不带引号的。 #可以防止sql注入
```

## 19.mysql 中的union与union All

```
UNION和UNION ALL关键字都是将两个结果集合并为一个，但这两者从使用和效率上来说都有所不同。

UNION=distinct+order by

1、对重复结果的处理：UNION在进行表链接后会筛选掉重复的记录，Union All不会去除重复记录。

2、对排序的处理：Union将会按照字段的顺序进行排序；UNION ALL只是简单的将两个结果合并后就返回。

从效率上说，UNION ALL 要比UNION快很多，所以，如果可以确认合并的两个结果集中不包含重复数据且不需要排序时的话，那么建议使用union all.

Every derived table must have its own alias（sql语句错误解决方法）：
	在做多表查询，或者查询的时候产生新的表的时候会出现这个错误：Every derived table must have its own alias（每一个派生出来的表都必须有一个自己的别名）
```

## ***20.数据库优化***

```
1.explain数据查看

   explain select id,name,code,phone,type from(
    select id ,emp_name as name, emp_code as code, emp_phone as phone,1 as type
    from t_courier_dtl where division_code="755DM" and is_valid= '1'
    union 
    select
    id, dept_name  as name, dept_code as code ,null as phone,2 as type
    from t_cust_overview   where division_code="755DM" and is_valid= '1'
    ) as a where name like "%丁%" or code like"1"  order by name desc limit 1,7;
```





```sql
各个含义:
1.id:表示select的标识符。
	SQL执行的顺序的标识，SQL从大到小的执行
	1. id相同时，执行顺序由上至下
    2. 如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行
    3. id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行
    
2.select_type:表示查询的类型	
	(1) SIMPLE(简单SELECT，不使用UNION或子查询等)
    (2) PRIMARY(子查询中最外层查询，查询中若包含任何复杂的子部分，最外层的select被标记为PRIMARY)
    (3) UNION(UNION中的第二个或后面的SELECT语句)
    (4) DEPENDENT UNION(UNION中的第二个或后面的SELECT语句，取决于外面的查询)
    (5) UNION RESULT(UNION的结果，union语句中第二个select开始后面所有select)
    (6) SUBQUERY(子查询中的第一个SELECT，结果不依赖于外部查询)
    (7) DEPENDENT SUBQUERY(子查询中的第一个SELECT，依赖于外部查询)
    (8) DERIVED(派生表的SELECT, FROM子句的子查询)
    (9) UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行)	
    
    1、simple ——简单的select查询，查询中不包含子查询或者UNION
    2、primary ——查询中若包含任何复杂的子部分，最外层查询被标记
    3、subquery——在select或where列表中包含了子查询
    4、derived——在from列表中包含的子查询被标记为derived（衍生），MySQL会递归执行这些子查询，把结果放到临时表中
    5、union——如果第二个select出现在UNION之后，则被标记为UNION，如果union包含在from子句的子查询中，外层select被标记为derived
    6、union result:UNION 的结果
    
3.table:输出结果集的表
	显示这一步所访问数据库中表名称（显示这一行的数据是关于哪张表的），有时不是真实的表名字，可能是简称，例如上面的e，d，也可能是第几步执行的结果的简称
	
4.partitions:匹配的分区

5.type:表示表的连接类型  一般保证查询至少达到range级别，最好能达到ref。
    对表访问方式，表示MySQL在表中找到所需行的方式，又称“访问类型”。
    常用的类型有： ALL、index、range、 ref、eq_ref、const、system、NULL（从左到右，性能从差到好）
    
    ALL：Full Table Scan， MySQL将遍历全表以找到匹配的行 ->全表查找
    
    index: Full Index Scan，index与ALL区别为index类型只遍历索引树  ->查找索引树
    
    range:只检索给定范围的行，使用一个索引来选择行  ，只检索给定范围的行，使用一个索引来选择行。key列显示使用了哪个索引。key_len包含所使用索引的最长关键元素。在该类型中ref列为NULL。当使用=、<>、>、>=、<、<=、IS 
NULL、<=>、BETWEEN或者IN操作符，用常量比较关键字列时，可以使用range           				 ->使用索引，一个范围
    
    index_subquery：该联接类型类似于unique_subquery。可以替换IN子查询，但只适合下列形式的子查询中的非唯一索引：value 
IN(SELECT key_column FROM single_table WHERE some_expr) 
    
    unique_subquery：该类型替换了下面形式的IN子查询的ref（in 的里面是主键）：value IN (SELECT primary_key 
FROMsingle_table WHERE some_expr);unique_subquery是一个索引查找函数，可以完全替换子查询，效率更高。 
    
    index_merge：该联接类型表示使用了索引合并优化方法。在这种情况下，key列包含了使用的索引的清单，key_len包含了使用的索引的最长的关键元素。
    
    ref_or_null：该联接类型如同ref，但是添加了MySQL可以专门搜索包含NULL值的行。在解决子查询中经常使用该联接类型的优化。 
    
    ref: 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值  
    对于每个来自于前面的表的行组合，所有有匹配索引值的行将从这张表中读取。如果联接只使用键的最左边的前缀，或如果键不是UNIQUE或PRIMARY KEY（换句话说，如果联接不能基于关键字选择单个行的话），则使用ref。如果使用的键仅仅匹配少量行，该联接类型是不错的。ref可以用于使用=或<=>操作符的带索引的列。                                           ->使用索引 并且不是唯一或者主键索引
    
    eq_ref: 类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件                       				->使用唯一索引或者主键作为连接条件
    对于每个来自于前面的表的行组合，从该表中读取一行。这可能是最好的联接类型，除了const类型。它用在一个索引的所有部分被联接使用并且索引是UNIQUE或PRIMARY KEY。eq_ref可以用于使用= 操作符比较的带索引的列。比较值可以为常量或一个使用在该表前面所读取的表的列的表达式。 
     
    const、system: 当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，MySQL就能将该查询转换为一个常量，system是const类型的特例，当查询的表只有一行的情况下，使用system        ->使用主键 仅仅只有一行数据
   例子： explain select distinct dept_name from t_courier_dtl where is_valid= '1' and id=2
   
    NULL: MySQL在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。
    																		->可能不用查找表，通过索引完成查找
    例子；explain select min(id)  from t_courier_dtl

6.possible_keys:表示查询时，可能使用的索引
    指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用（该查询可以利用的索引，如果没有任何索引显示 null）

    该列完全独立于EXPLAIN输出所示的表的次序。这意味着在possible_keys中的某些键实际上不能按生成的表次序使用。
    如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查WHERE子句看是否它引用某些列或适合索引的列来提高你的查询性能。如果是这样，创造一个适当的索引并且再次用EXPLAIN检查查询

7.key:表示实际使用的索引
    key列显示MySQL实际决定使用的键（索引），有可能不会出现在possible_keys中(这时可能用的是覆盖索引，即使query中没有where)。possible_keys揭示哪个索引更有效，key是优化器决定哪个索引可能最小化查询成本，查询成本基于系统开销等总和因素，有可能是“执行时间”矛盾。如果强制mysql使用或者忽略possible_keys中的索引，需要在query中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。
    
8.key_len:索引字段的长度
    表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的）
    不损失精确性的情况下，长度越短越好 
    比如：ColumnA(char(3)) ColumnB(int(11))，在utf-8的字符集下，key_len=3*3+4=13。计算该值时需要考虑字符列对应的字符集，不同字符集对应不同的字节数。 
    
9.ref:列与索引的比较
	显示了哪些字段或者常量被用来和 key 配合从表中查询记录出来。显示那些在index查询中被当作值使用的在其他表里的字段或者constants。

10.rows:扫描出的行数(估算的行数)
	估算出结果集行数，表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数
	
11.filtered:按表条件过滤的行百分比

12.Extra:执行情况的描述和说明
    该列包含MySQL解决查询的详细信息。 
    1、Distinct：MySQL发现第1个匹配行后，停止为当前的行组合搜索更多的行。 
    2、Not exists：MySQL能够对查询进行LEFT JOIN优化，发现1个匹配LEFT JOIN标准的行后，不再为前面的的行组合在该表内检查更多的行。 
    3、range checked for each record (index map: #)：MySQL没有发现好的可以使用的索引，但发现如果来自前面的表的列值已知，可能部分索引可以使用。对前面的表的每个行组合，MySQL检查是否可以使用range或index_merge访问方法来索取行。 
    4、Using filesort：MySQL需要额外的一次传递，以找出如何按排序顺序检索行。通过根据联接类型浏览所有行并为所有匹配WHERE子句的行保存排序关键字和行的指针来完成排序。然后关键字被排序，并按排序顺序检索行。 
    5、Using index：从只使用索引树中的信息而不需要进一步搜索读取实际的行来检索表中的列信息。当查询只使用作为单一索引一部分的列时，可以使用该策略。 
    6、Using temporary：为了解决查询，MySQL需要创建一个临时表来容纳结果。典型情况如查询包含可以按不同情况列出列的GROUP BY和ORDER BY子句时。 
    7、Using where：WHERE子句用于限制哪一个行匹配下一个表或发送到客户。除非你专门从表中索取或检查所有行，如果Extra值不为Using where并且表联接类型为ALL或index，查询可能会有一些错误。 
    8、Using sort_union(…), Using union(…), Using intersect(…)：这些函数说明如何为index_merge联接类型合并索引扫描。 
    9、Using index for group-by：类似于访问表的Using index方式，Using index for group-by表示MySQL发现了一个索引，可以用来查询GROUP BY或DISTINCT查询的所有列，而不要额外搜索硬盘访问实际的表。并且，按最有效的方式使用索引，以便对于每个组，只读取少量索引条目。
    
总结：
• EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况
• EXPLAIN不考虑各种Cache
• EXPLAIN不能显示MySQL在执行查询时所作的优化工作
• 部分统计信息是估算的，并非精确值
• EXPALIN只能解释SELECT操作，其他操作要重写为SELECT后查看执行计划。



2.sql执行顺序：
            select distinct 
                    <select_list>
            from
                <left_table><join_type>
            join <right_table> on <join_condition>
            where
                <where_condition>
            group by
                <group_by_list>
            having
                <having_condition>
            order by
                <order_by_condition>
            limit <limit number>
顺序：
        1、from <left_table><join_type>
        2、on <join_condition>
        3、<join_type> join <right_table>
        4、where <where_condition>
        5、group by <group_by_list>
        6、having <having_condition>
        7、select
        8、distinct <select_list>
        9、order by <order_by_condition>
        10、limit <limit_number>
        
        
2.尽量不要在循环语句中进行数据库查询，可以拼接一下，搞成in  可以减少数据库循环链接查找的损耗


mysql的高水位线：https://blog.csdn.net/leshami/article/details/6949179

随着记录的不断增加，新块不断地被填充并使用，高水位线随之向右移动。高水位线之上为未格式化的数据块。
  删除(delete)操作之后，高水位线之下的块处于空闲状态，但高水位线并不随之下降，直到重建，截断或收缩表段。
  全表扫描会扫描高水位线之下的所有块，包括空闲数据块(执行了delete操作)。


1、高水线直接决定了全表扫描所需要的I/O开销
  2、delete操作不会降低高水位线，高水位线之下的所有块依然被扫描
  3、使用truncate 会重置高水位线到0位
  4、定期使用alter table tab_name shrink space cascade 有效减少该对象上的I/O开销
  
  
mysql高水位线问题：

删除数据、

插入数据索引不连续带来的页内数据空洞，表实际占用空间增大
优化：

alter table table_name  engine = InnoDB

相当于建立临时表，把表删除后,重新插入数据

原理：

InnoDB引擎只会把这个记录标记为删除，如果要复用，必须要插入相同id的，否则空间不会回收

如果删除的是page页，页的复用没有对id相同的要求
```



![SQLè§£æ](https://img-blog.csdn.net/20180729102302530?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYWRhamluZzI2Nw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## 21.常用sql语句

```
1.union 	
    explain select id,name,code,phone,type from(

    select id ,emp_name as name, emp_code as code, emp_phone as phone,1 as type
    from t_courier_dtl where division_code="755DM" and is_valid= '1'

    union 

    select
    id, dept_name  as name, dept_code as code ,null as phone,2 as type
    from t_cust_overview   where division_code="755DM" and is_valid= '1'

    ) as a where name like "%丁%" or code like"1"  order by name desc limit 1,7;
2.查找分组最大计数
select division_code,count(division_code)
from t_courier_dtl GROUP BY(division_code) HAVING (count(division_code)>10)
3.建新表并且将数据迁移
    create table ts_role_1020 like ts_role;
    insert into ts_role_1020
    select * from ts_role;
```

## 22.Mybatis

```java
1.mybatis的选择语句  choose when:相当于if else switch

select count(*) from(
        <choose>
            <when test="divisionCode != null and divisionCode != ''">
                select id ,emp_name as name, emp_code as code,1 as type
                from t_courier_dtl where division_code=#{divisionCode} and is_valid= '1' union
                select id, dept_name as name, dept_code as code ,2 as type
                from t_cust_overview where division_code=#{divisionCode} and is_valid= '1'
            </when>
            <otherwise>
                select id ,emp_name as name, emp_code as code,1 as type
                from t_courier_dtl where dept_code=#{deptCode} and is_valid= '1'
            </otherwise>
        </choose>
        ) as a where name like concat('%',#{searchField},'%') or code like concat('%',#{searchField},'%') order by name
2.if语句：判断if
  <select id="dynamicIfTest" parameterType="Blog" resultType="Blog">
        select * from t_blog where 1 = 1
        <if test="title != null">
            and title = #{title}
        </if>
        <if test="content != null">
            and content = #{content}
        </if>
        <if test="owner != null">
            and owner = #{owner}
        </if>
    </select>
3.trim (对包含的内容加上 prefix,或者 suffix 等，前缀，后缀) 
<select id="dynamicTrimTest" parameterType="Blog" resultType="Blog">
        select * from t_blog 
        <trim prefix="where" prefixOverrides="and |or">
            <if test="title != null">
                title = #{title}
            </if>
            <if test="content != null">
                and content = #{content}
            </if>
            <if test="owner != null">
                or owner = #{owner}
            </if>
        </trim>
    </select>
4.where: 主要是用来简化sql语句中where条件判断的，能智能的处理 and or 条件
<select id="dynamicWhereTest" parameterType="Blog" resultType="Blog">
        select * from t_blog 
        <where>
            <if test="title != null">
                title = #{title}
            </if>
            <if test="content != null">
                and content = #{content}
            </if>
            <if test="owner != null">
                and owner = #{owner}
            </if>
        </where>
    </select>
    where元素的作用是会在写入where元素的地方输出一个where，另外一个好处是你不需要考虑where元素里面的条件输出是什么样子的，MyBatis会智能的帮你处理，如果所有的条件都不满足那么MyBatis就会查出所有的记录，如果输出后是and 开头的，MyBatis会把第一个and忽略，当然如果是or开头的，MyBatis也会把它忽略；
5.set:更新值  tips:  set会智能去除sql修改语句的最后一个逗号
 <update id="dynamicSetTest" parameterType="Blog">
        update t_blog
        <set>
            <if test="title != null">
                title = #{title},
            </if>
            <if test="content != null">
                content = #{content},
            </if>
            <if test="owner != null">
                owner = #{owner}
            </if>
        </set>
        where id = #{id}
    </update>
6.逆向工程
			输入该参数生成文件：mybatis-generator:generate -e  mybatis-generator:generate
                
7.原理:https://blog.csdn.net/weixin_43184769/article/details/91126687

	MappedStatement：存储Mapper节点下面的数据 主要是描述一条sql
    Configuration：保存xml中的所有属性


    public static void main(String[] args) {
            InputStream inputStream = Resources.getResourceAsStream("mybatis-config.xml");//读取配置
            SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(inputStream);//赋值属性
            SqlSession sqlSession = factory.openSession();//设置属性 事务隔离级别 事务是否自动提交
            String name = "tom";
            List<User> list = sqlSession.selectList("com.demo.mapper.UserMapper.getUserByName",params);//运行sql语句
    }
1.创建SqlSessionFactoryBuilder对象，调用build(inputstream)方法读取并解析配置文件，返回SqlSessionFactory对象
2.由SqlSessionFactory创建SqlSession 对象，没有手动设置的话事务默认开启
3.调用SqlSession中的api，传入Statement Id和参数，内部进行复杂的处理，最后调用jdbc执行SQL语句，封装结果返回。
    
    其中：初始化配置文件信息的本质就是创建Configuration对象，将解析的xml数据封装到Configuration内部的属性中。
    
    //使用mapper
    public static void main(String[] args) {
		//前三步都相同
		InputStream inputStream = Resources.getResourceAsStream("mybatis-config.xml");
		SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(inputStream);
		SqlSession sqlSession = factory.openSession();
		//这里不再调用SqlSession 的api，而是获得了接口对象，调用接口中的方法。
		UserMapper mapper = sqlSession.getMapper(UserMapper.class);
		List<User> list = mapper.getUserByName("tom");
	}

第一种情况源码解析：	
 1.SqlSessionFactoryBuilder().build(inputStream)：build方法解析
    // 1.我们最初调用的build
    public SqlSessionFactory build(InputStream inputStream) {
        //调用了重载方法
        return build(inputStream, null, null);
      }

    // 2.调用的重载方法  第一次重载
    public SqlSessionFactory build(InputStream inputStream, String environment, Properties properties) {
        try {
          //  XMLConfigBuilder是专门解析mybatis的配置文件的类  将xml中通文件解析
          XMLConfigBuilder parser = new XMLConfigBuilder(inputStream, environment, properties);
          //这里又调用了一个重载方法。parser.parse()的返回值是Configuration对象
          return build(parser.parse());
        } catch (Exception e) {
          throw ExceptionFactory.wrapException("Error building SqlSession.", e);
        } //省略部分代码
      }
        2.1.XMLConfigBuilder作用:通过XMLConfigBuilder解析xml文件：即解析其中的标签  
              <configuration>
                <settings>
                    <setting name="logPrefix" value="mybatis.sql." />
                    <setting name="mapUnderscoreToCamelCase" value="true" />
                    <setting name="logImpl" value="STDOUT_LOGGING" />
                </settings>
            </configuration>
        其中，xml中的配置标签有：properties（属性），settings（设置），typeAliases（类型别名），typeHandlers（类型处理器），objectFactory（对象工厂），mappers（映射器）等
    
```

![image-20200913142951217](d:\user\01397358\Application Data\Typora\typora-user-images\image-20200913142951217.png)

```java
	XMLConfigBuilder解析xml文件就是标签，
    作用：初始化配置文件信息的本质就是创建Configuration对象，将解析的xml数据封装到Configuration内部的属性中。
    然后执行其parse（）方法：
 	//在创建XMLConfigBuilder时，它的构造方法中解析器XPathParser已经读取了配置文件
    //2.2. 进入XMLConfigBuilder 中的 parse()方法。
    public Configuration parse() {
        if (parsed) {
          throw new BuilderException("Each XMLConfigBuilder can only be used once.");
        }
        parsed = true;
        //parser是XPathParser解析器对象，读取节点内数据，<configuration>是MyBatis配置文件中的顶层标签
        parseConfiguration(parser.evalNode("/configuration"));
        //最后返回的是Configuration 对象
        return configuration;
    }

configuration：
 protected final MapperRegistry mapperRegistry = new MapperRegistry(this);
  protected final InterceptorChain interceptorChain = new InterceptorChain();
  protected final TypeHandlerRegistry typeHandlerRegistry = new TypeHandlerRegistry();
  protected final TypeAliasRegistry typeAliasRegistry = new TypeAliasRegistry();
  protected final LanguageDriverRegistry languageRegistry = new LanguageDriverRegistry();

  protected final Map<String, MappedStatement> mappedStatements = new StrictMap<MappedStatement>("Mapped Statements collection");
  protected final Map<String, Cache> caches = new StrictMap<Cache>("Caches collection");
  protected final Map<String, ResultMap> resultMaps = new StrictMap<ResultMap>("Result Maps collection");
  protected final Map<String, ParameterMap> parameterMaps = new StrictMap<ParameterMap>("Parameter Maps collection");
  protected final Map<String, KeyGenerator> keyGenerators = new StrictMap<KeyGenerator>("Key Generators collection");


    //2.3. 进入parseConfiguration方法
    //此方法中读取了各个标签内容并封装到Configuration中的属性中。
    private void parseConfiguration(XNode root) {
        try {
          //issue #117 read properties first
          propertiesElement(root.evalNode("properties"));
          Properties settings = settingsAsProperties(root.evalNode("settings"));
          loadCustomVfs(settings);
          loadCustomLogImpl(settings);
          typeAliasesElement(root.evalNode("typeAliases"));
          pluginElement(root.evalNode("plugins"));
          objectFactoryElement(root.evalNode("objectFactory"));
          objectWrapperFactoryElement(root.evalNode("objectWrapperFactory"));
          reflectorFactoryElement(root.evalNode("reflectorFactory"));
          settingsElement(settings);
          // read it after objectFactory and objectWrapperFactory issue #631
          environmentsElement(root.evalNode("environments"));
          databaseIdProviderElement(root.evalNode("databaseIdProvider"));
          typeHandlerElement(root.evalNode("typeHandlers"));
          mapperElement(root.evalNode("mappers"));
        } catch (Exception e) {
          throw new BuilderException("Error parsing SQL Mapper Configuration. Cause: " + e, e);
        }
    }

3.到此对xml配置文件的解析就结束了，回到步骤 2. 中调用的重载build方法。 第二次重载
 //build最后的重载方法
 public SqlSessionFactory build(Configuration config) {
	//创建了DefaultSqlSessionFactory对象，传入Configuration对象。
    return new DefaultSqlSessionFactory(config);
  }
4.factory.openSession()方法详解：
    sqlSession是一个接口，它有两个实现类：DefaultSqlSession（默认）和SqlSessionManager（弃用，不做介绍）
SqlSession是MyBatis中用于和数据库交互的顶层类，通常将它与ThreadLocal绑定，一个会话使用一个SqlSession，并且在使用完毕后需要close。
    
public interface SqlSession extends Closeable:该接口定义了一系列的操作
      <T> T selectOne(String statement);
      <T> T selectOne(String statement, Object parameter);
      <E> List<E> selectList(String statement);
      <E> List<E> selectList(String statement, Object parameter);

  private final Configuration configuration;：解析xml的类
  private final Executor executor;：执行器
SqlSession中的两个最重要的参数，configuration与初始化时的相同，Executor为执行器:
   configuration与初始化时的相同,保存解析xml文件的一些值。
   executor：   
      Executor也是一个接口，他有三个常用的实现类BatchExecutor（重用语句并执行批量更新），ReuseExecutor（重用预处理语句prepared statements），SimpleExecutor（普通的执行器，默认）。
       
   通过OpenSession（）方法获取sqlsession：
       
       //6. 进入openSession方法。
      public SqlSession openSession() {
        //getDefaultExecutorType()传递的是SimpleExecutor
        return openSessionFromDataSource(configuration.getDefaultExecutorType(), null, false);
      }

    //7. 进入openSessionFromDataSource。 类型+事务+自动提交
    //ExecutorType 为Executor的类型，TransactionIsolationLevel为事务隔离级别，autoCommit是否开启事务
    //openSession的多个重载方法可以指定获得的SeqSession的Executor类型和事务的处理
    private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) {
        Transaction tx = null;
        try {
          final Environment environment = configuration.getEnvironment();
          final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment);
          tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit);
          //根据参数创建指定类型的Executor
          final Executor executor = configuration.newExecutor(tx, execType);
          //返回的是DefaultSqlSession
          return new DefaultSqlSession(configuration, executor, autoCommit);
        } catch (Exception e) {
          closeTransaction(tx); // may have fetched a connection so lets call close()
          throw ExceptionFactory.wrapException("Error opening session.  Cause: " + e, e);
        } finally {
          ErrorContext.instance().reset();
        }
      }

 执行sqlsession中的api：    
     执行代码：
     SqlSession sqlSession = factory.openSession();
	String name = "tom";
	List<User> list = sqlSession.selectList("com.demo.mapper.UserMapper.getUserByName",params);

	源码：
       //8.进入selectList方法，多个重载方法。
    public <E> List<E> selectList(String statement) {
        return this.selectList(statement, null);
    }
    public <E> List<E> selectList(String statement, Object parameter) {
        return this.selectList(statement, parameter, RowBounds.DEFAULT);
    }

    public <E> List<E> selectList(String statement, Object parameter, RowBounds rowBounds) {
        try {
          //根据传入的全限定名+方法名从映射的Map中取出MappedStatement对象
          MappedStatement ms = configuration.getMappedStatement(statement);
          //调用Executor中的方法处理
          return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER);
        } catch (Exception e) {
          throw ExceptionFactory.wrapException("Error querying database.  Cause: " + e, e);
        } finally {
          ErrorContext.instance().reset();
        }
     }

介绍一下MappedStatement ：封装Mapper标签中的所有标签。 与sql相关
作用： MappedStatement与Mapper配置文件中的一个select/update/insert/delete节点相对应。mapper中配置的标签都被封装到了此对象中，主要用途是描述一条SQL语句。
**初始化过程：**回顾刚开始介绍的加载配置文件的过程中，会对mybatis-config.xml中的各个标签都进行解析，其中有 mappers标签用来引入mapper.xml文件或者配置mapper接口的目录。
   
    <select id="getUser" resultType="user" >
    select * from user where id=#{id}
  </select>
  
   这样的一个select标签会在初始化配置文件时被解析封装成一个MappedStatement对象，然后存储在Configuration对象的mappedStatements属性中，mappedStatements 是一个HashMap，存储时key = 全限定类名 + 方法名，value = 对应的MappedStatement对象。
  在configuration中对应的属性为：
protected final Map<String, MappedStatement> mappedStatements = new StrictMap<MappedStatement>("Mapped Statements collection");
  在XMLConfigBuilder中的处理：存储标签的值
      private void parseConfiguration(XNode root) {
      try {
          // 省略其他标签的处理
          mapperElement(root.evalNode("mappers"));
      } catch (Exception e) {
          throw new BuilderException("Error parsing SQL Mapper Configuration. Cause: " + e, e);
      }
  }

继续源码中的步骤，进入 executor.query()：
    //此方法在SimpleExecutor的父类BaseExecutor中实现
public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException {
	//根据传入的参数动态获得SQL语句，最后返回用BoundSql对象表示
    BoundSql boundSql = ms.getBoundSql(parameter);
    //为本次查询创建缓存的Key
    CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql);
    return query(ms, parameter, rowBounds, resultHandler, key, boundSql);
 }
 
//进入query的重载方法中
public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException {
    ErrorContext.instance().resource(ms.getResource()).activity("executing a query").object(ms.getId());
    if (closed) {
      throw new ExecutorException("Executor was closed.");
    }
    if (queryStack == 0 && ms.isFlushCacheRequired()) {
      clearLocalCache();
    }
    List<E> list;
    try {
      queryStack++;
      list = resultHandler == null ? (List<E>) localCache.getObject(key) : null;
      if (list != null) {
        handleLocallyCachedOutputParameters(ms, key, parameter, boundSql);
      } else {
      	// 如果缓存中没有本次查找的值，那么从数据库中查询
        list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);
      }
    } finally {
      queryStack--;
    }
    if (queryStack == 0) {
      for (DeferredLoad deferredLoad : deferredLoads) {
        deferredLoad.load();
      }
      // issue #601
      deferredLoads.clear();
      if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) {
        // issue #482
        clearLocalCache();
      }
    }
    return list;
  }

//从数据库查询
private <E> List<E> queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException {
    List<E> list;
    localCache.putObject(key, EXECUTION_PLACEHOLDER);
    try {
      // 查询的方法
      list = doQuery(ms, parameter, rowBounds, resultHandler, boundSql);
    } finally {
      localCache.removeObject(key);
    }
    // 将查询结果放入缓存
    localCache.putObject(key, list);
    if (ms.getStatementType() == StatementType.CALLABLE) {
      localOutputParameterCache.putObject(key, parameter);
    }
    return list;
  }

// SimpleExecutor中实现父类的doQuery抽象方法
public <E> List<E> doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException {
    Statement stmt = null;
    try {
      Configuration configuration = ms.getConfiguration();
      // 传入参数创建StatementHanlder对象来执行查询
      StatementHandler handler = configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql);
      // 创建jdbc中的statement对象
      stmt = prepareStatement(handler, ms.getStatementLog());
      // StatementHandler进行处理
      return handler.query(stmt, resultHandler);
    } finally {
      closeStatement(stmt);
    }
  }

// 创建Statement的方法
private Statement prepareStatement(StatementHandler handler, Log statementLog) throws SQLException {
    Statement stmt;
    //条代码中的getConnection方法经过重重调用最后会调用openConnection方法，从连接池中获得连接。
    Connection connection = getConnection(statementLog);
    stmt = handler.prepare(connection, transaction.getTimeout());
    handler.parameterize(stmt);
    return stmt;
  }
//从连接池获得连接的方法
protected void openConnection() throws SQLException {
    if (log.isDebugEnabled()) {
      log.debug("Opening JDBC Connection");
    }
    //从连接池获得连接
    connection = dataSource.getConnection();
    if (level != null) {
      connection.setTransactionIsolation(level.getLevel());
    }
    setDesiredAutoCommit(autoCommit);
  }

//进入StatementHandler进行处理的query，StatementHandler中默认的是PreparedStatementHandler
public <E> List<E> query(Statement statement, ResultHandler resultHandler) throws SQLException {
    PreparedStatement ps = (PreparedStatement) statement;
    //原生jdbc的执行
    ps.execute();
    //处理结果返回。
    return resultSetHandler.handleResultSets(ps);
  }
以上是直接通过
```

```java
第二种源码解析：使用mapper

通常的Mapper接口我们都没有实现的方法却可以使用，是为什么呢？答案很简单 使用动态代理

开始之前介绍一下MyBatis初始化时对接口的处理：MapperRegistry是Configuration中的一个属性，它内部维护一个HashMap用于存放mapper接口的工厂类，每个接口对应一个工厂类。mappers中可以配置接口的包路径，或者某个具体的接口类。

<!-- 将包内的映射器接口实现全部注册为映射器 -->
<mappers>
  <mapper class="com.demo.mapper.UserMapper"/>
  <package name="com.demo.mapper"/>
</mappers>

当解析mappers标签时，它会判断解析到的是mapper配置文件时，会再将对应配置文件中的增删改查标签一 一封装成MappedStatement对象，存入mappedStatements中。（上文介绍了）
当判断解析到接口时，会创建此接口对应的MapperProxyFactory对象，存入HashMap中，key = 接口的字节码对象，value = 此接口对应的MapperProxyFactory对象。

注册时：以接口类的class对象为key，value为其对应的工厂对象，构造方法中指定了接口对象      
    //MapperRegistry类
    public class MapperRegistry {
      private final Configuration config;
      //这个类中维护一个HashMap存放MapperProxyFactory
      private final Map<Class<?>, MapperProxyFactory<?>> knownMappers = new HashMap<>();

      //解析到接口时添加接口工厂类的方法
      public <T> void addMapper(Class<T> type) {
        if (type.isInterface()) {
          if (hasMapper(type)) {
            throw new BindingException("Type " + type + " is already known to the MapperRegistry.");
          }
          boolean loadCompleted = false;
          try {
            //重点在这行，以接口类的class对象为key，value为其对应的工厂对象，构造方法中指定了接口对象
            knownMappers.put(type, new MapperProxyFactory<>(type));
    
            MapperAnnotationBuilder parser = new MapperAnnotationBuilder(config, type);
            parser.parse();
            loadCompleted = true;
          } finally {
            if (!loadCompleted) {
              knownMappers.remove(type);
            }
          }
        }
      }
    }

进入sqlSession.getMapper(UserMapper.class)中:
    //DefaultSqlSession中的getMapper
    public <T> T getMapper(Class<T> type) {
        return configuration.<T>getMapper(type, this);
    }

    //configuration中的给getMapper
    public <T> T getMapper(Class<T> type, SqlSession sqlSession) {
        return mapperRegistry.getMapper(type, sqlSession);
    }

    //MapperRegistry中的getMapper
    public <T> T getMapper(Class<T> type, SqlSession sqlSession) {
        //从MapperRegistry中的HashMap中拿MapperProxyFactory
        final MapperProxyFactory<T> mapperProxyFactory = (MapperProxyFactory<T>) knownMappers.get(type);
        if (mapperProxyFactory == null) {
          throw new BindingException("Type " + type + " is not known to the MapperRegistry.");
        }
        try {
          // 通过动态代理工厂生成示例。
          return mapperProxyFactory.newInstance(sqlSession);
        } catch (Exception e) {
          throw new BindingException("Error getting mapper instance. Cause: " + e, e);
        }
    }



    //MapperProxyFactory类中的newInstance方法
     public T newInstance(SqlSession sqlSession) {
        // 创建了JDK动态代理的Handler类
        final MapperProxy<T> mapperProxy = new MapperProxy<>(sqlSession, mapperInterface, methodCache);
        // 调用了重载方法
        return newInstance(mapperProxy);
      }

    //MapperProxy类，实现了InvocationHandler接口
    public class MapperProxy<T> implements InvocationHandler, Serializable {

      //省略部分源码	

      private final SqlSession sqlSession;
      private final Class<T> mapperInterface;
      private final Map<Method, MapperMethod> methodCache;

      // 构造，传入了SqlSession，说明每个session中的代理对象的不同的！
      public MapperProxy(SqlSession sqlSession, Class<T> mapperInterface, Map<Method, MapperMethod> methodCache) {
        this.sqlSession = sqlSession;
        this.mapperInterface = mapperInterface;
        this.methodCache = methodCache;
      }

      //省略部分源码
    }

    //重载的方法，由动态代理创建新示例返回。
    protected T newInstance(MapperProxy<T> mapperProxy) {
        return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] { mapperInterface }, mapperProxy);
     }

在动态代理返回了示例后，我们就可以直接调用mapper类中的方法了，说明在MapperProxy中的invoke方法中已经为我们实现了方法。
    
    
     public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        try {
          //判断调用是是不是Object中定义的方法，toString，hashCode这类非。是的话直接放行。
          if (Object.class.equals(method.getDeclaringClass())) {
            return method.invoke(this, args);
          } else if (isDefaultMethod(method)) {
            return invokeDefaultMethod(proxy, method, args);
          }
        } catch (Throwable t) {
          throw ExceptionUtil.unwrapThrowable(t);
        } 
        final MapperMethod mapperMethod = cachedMapperMethod(method);
        // 重点在这：MapperMethod最终调用了执行的方法
        return mapperMethod.execute(sqlSession, args);
      }


    public Object execute(SqlSession sqlSession, Object[] args) {
        Object result;
        //判断mapper中的方法类型，最终调用的还是SqlSession中的方法
        switch (command.getType()) {
          case INSERT: {
            Object param = method.convertArgsToSqlCommandParam(args);
            result = rowCountResult(sqlSession.insert(command.getName(), param));
            break;
          }
          case UPDATE: {
            Object param = method.convertArgsToSqlCommandParam(args);
            result = rowCountResult(sqlSession.update(command.getName(), param));
            break;
          }
          case DELETE: {
            Object param = method.convertArgsToSqlCommandParam(args);
            result = rowCountResult(sqlSession.delete(command.getName(), param));
            break;
          }
          case SELECT:
            if (method.returnsVoid() && method.hasResultHandler()) {
              executeWithResultHandler(sqlSession, args);
              result = null;
            } else if (method.returnsMany()) {
              result = executeForMany(sqlSession, args);
            } else if (method.returnsMap()) {
              result = executeForMap(sqlSession, args);
            } else if (method.returnsCursor()) {
              result = executeForCursor(sqlSession, args);
            } else {
              Object param = method.convertArgsToSqlCommandParam(args);
              result = sqlSession.selectOne(command.getName(), param);
              if (method.returnsOptional() &&
                  (result == null || !method.getReturnType().equals(result.getClass()))) {
                result = Optional.ofNullable(result);
              }
            }
            break;
          case FLUSH:
            result = sqlSession.flushStatements();
            break;
          default:
            throw new BindingException("Unknown execution method for: " + command.getName());
        }
        if (result == null && method.getReturnType().isPrimitive() && !method.returnsVoid()) {
          throw new BindingException("Mapper method '" + command.getName()
              + " attempted to return null from a method with a primitive return type (" + method.getReturnType() + ").");
        }
        return result;
      }
```



## 23.B树与B+树

## 24.assembly

```xml
assembly:帮助打包使用  支持个性化的打包方式

	1.pom文件插件添加：
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-assembly-plugin</artifactId>
				<configuration>
					<appendAssemblyId>false</appendAssemblyId>
					<descriptors>
						<descriptor>src/main/assembly/assembly.xml</descriptor>//配置文件
					</descriptors>
					<skipAssembly>false</skipAssembly>
					<finalName>${project.artifactId}</finalName>
				</configuration>
				<executions>
					<execution>
						<id>make-assembly-uniform</id>
						<phase>package</phase>
						<goals>
							<goal>single</goal>//运行一次
						</goals>
					</execution>
				</executions>
			</plugin>
   说明:1.需要修改的可能就是descriptors标签下面的打包配置文件目录,指定assembly.xml的路径.
　　　　2.可以添加多个打包配置文件,进行多种形式打包,比如添加assembly2.xml路径
　　　　
2.assembly.xml,完整的打包配置

<assembly
	xmlns="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0 http://maven.apache.org/xsd/assembly-1.1.0.xsd">
	<id>assembly-uniform-task</id>
	<formats>
		<format>zip</format>
	</formats>
	<includeBaseDirectory>false</includeBaseDirectory>
	<fileSets>
		<fileSet>
			<directory>${project.build.directory}/dubbo/META-INF/assembly/bin</directory>
			<excludes>
				<exclude>start.sh</exclude>
			</excludes>
			<outputDirectory>bin</outputDirectory>
			<directoryMode>0755</directoryMode>
			<fileMode>0755</fileMode>
		</fileSet>
		<fileSet>
			<directory>src/main/assembly/bin</directory>
			<outputDirectory>bin</outputDirectory>
			<directoryMode>0755</directoryMode>
			<fileMode>0755</fileMode>
		</fileSet>
		<fileSet>
			<directory>src/main/assembly/${env.devMode}/conf</directory>
			<outputDirectory>conf</outputDirectory>
			<directoryMode>0744</directoryMode>
			<fileMode>0644</fileMode>
		</fileSet>
	</fileSets>
	<dependencySets>
		<dependencySet>
			<useProjectArtifact>true</useProjectArtifact>
			<outputDirectory>lib</outputDirectory>
			<scope>runtime</scope>
		</dependencySet>
	</dependencySets>
</assembly>
　说明:1.formats限定打包格式,支持常见多种格式,zip,gz什么的

　　　　2.filesets标签指定要打包的目录.点对点打包,将某一个目录打包到某一个目录下,可以选择排除某些目录和包含某些目录

　　　　3.dependencySets标签指定将scope范围内的jar包打包到指定目录

　　　　4.moduleSets标签指定是否将项目下的木块打包到指定目录,可以选择部分目录

3.运行mvn package  进行打包
```

## 25.git（https://www.cnblogs.com/jjlee/p/10305194.html）

![img](https://img2018.cnblogs.com/blog/1588197/201901/1588197-20190129144117651-860073754.png)

Workspace：工作区
Index / Stage：暂存区
Repository：仓库区（或本地仓库）
Remote：远程仓库

```
git常用命令：
	git init 初始化一个git仓库
	git add file:git添加一个文件
	git commit -m'注释'：将文件commmit 并且添加注释
	git status:查询是否还有文件未提交
	git diff file:查看文件和之前的不同
	git log:查询git的日志
	git reset --hard HEAD^：版本回退到上个版本
	git reset --hard HEAD^^:版本回退到上上个版本
	git reset --hard HEAD~100:版本回退到100个版本
	git reset --hard 版本号：通过版本号回退
	git reflog ：获取版本号
	git commit:将所有文件提交到分支上
	git checkout -- file:丢弃工作区的修改  readme.txt文件在工作区做的修改全部撤销
	
	git branch 分支名，创建本地分支
	
	git checkout 分支名：切换分支  git checkout -b dev
	
	git push origin 2.0.1.20120806：push到远程服务器
	
	git checkout --track origin/2.0.1.20120806:拉取特定分支
	

	git fetch是将远程主机的最新内容拉到本地，用户在检查了以后决定是否合并到工作本机分支中。
	
	git checkout -- b.txt:将仓库的数据恢复
	
	冲突修改：
	先用git pull把最新的提交从origin/dev抓下来，然后在本地合并，解决冲突，再推送。
```

## 26.mock



## 27.Jenkins

```
Jenkins是一个开源的、提供友好操作界面的持续集成(CI)工具，起源于Hudson（Hudson是商用的），主要用于持续、自动的构建/测试软件项目、监控外部任务的运行（这个比较抽象，暂且写上，不做解释）。Jenkins用Java语言编写，可在Tomcat等流行的servlet容器中运行，也可独立运行。通常与版本管理工具(SCM)、构建工具结合使用。常用的版本控制工具有SVN、GIT，构建工具有Maven、Ant、Gradle。
```





## 28.代理方式

```
代理根据代理类的产生方式和时机分为静态代理和动态代理两种。代理类不仅可以有效的将具体的实现与调用方进行解耦，通过面向接口进行编码完全将具体的实现隐藏在内部，而且还可以在符合开闭原则的前提下，对目标类进行进一步的增强。典型地，Spring AOP 是对JDK动态代理的经典应用。

那么，静态代理与动态代理的区别是什么呢？所谓静态代理，其实质是自己手写(或者用工具生成)代理类，也就是在程序运行前就已经存在的编译好的代理类。但是，如果我们需要很多的代理，每一个都这么去创建实属浪费时间，而且会有大量的重复代码，此时我们就可以采用动态代理，动态代理可以在程序运行期间根据需要动态的创建代理类及其实例来完成具体的功能。总的来说，
根据代理类的创建时机和创建方式的不同，我们可以将代理分为静态代理和动态代理两种形式。

代理作用：
	1.代理对象存在的价值主要用于拦截对真实业务对象的访问
	2.代理对象应该具有和目标对象(真实业务对象)相同的方法，即实现共同的接口或继承于同一个类
	3.代理对象应该是目标对象的增强，否则我们就没有必要使用代理了
	
代理模式是较常见的模式之一，在许多框架中经常见到，比如Spring的面向切面的编程，MyBatis中缓存机制对PooledConnection的管理等。代理模式使得客户端在使用目标对象的时候间接通过操作代理对象进行，代理对象是对目标对象的增强，代理模式的UML示意图如下：

```

![img](https://img-blog.csdn.net/20170703163038672?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvanVzdGxvdmV5b3Vf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

```java
代理模式包含如下几个角色：

- 客户端：客户端面向接口编程，使用代理角色完成某项功能。
- 抽象主题：一般实现为接口，是对(被代理对象的)行为的抽象。
- 被代理角色(目标类)：直接实现上述接口，是抽象主题的具体实现。
- 代理角色(代理类)：实现上述接口，是对被代理角色的增强。

**代理模式的使用本质上是对开闭原则(Open for Extension, Close for Modification)的直接支持。**
    
静态代理：代理类之前存在
静态代理的实现模式一般是：首先创建一个接口（JDK代理都是面向接口的），然后创建具体实现类来实现这个接口，然后再创建一个代理类同样实现这个接口，不同之处在于，具体实现类的方法中需要将接口中定义的方法的业务逻辑功能实现，而代理类中的方法只要调用具体类中的对应方法即可，这样我们在需要使用接口中的某个方法的功能时直接调用代理类的方法即可，将具体的实现类隐藏在底层。

例子：
    
	接口：
    public interface Movie {
        void play();
    }

    实现类：
    public class RealMovie implements Movie {
        @Override
        public void play() {
        // TODO Auto-generated method stub
        System.out.println("您正在观看电影 《肖申克的救赎》");
        }
    }

    代理类：
    public class Cinema implements Movie {
        RealMovie movie;
        public Cinema(RealMovie movie) {
            super();
            this.movie = movie;
        }
        @Override
        public void play() {
            guanggao(true);    // 代理类的增强处理
            movie.play();     // 代理类把具体业务委托给目标类，并没有直接实现
            guanggao(false);    // 代理类的增强处理
        }

        public void guanggao(boolean isStart){
            if ( isStart ) {
                System.out.println("电影马上开始了，爆米花、可乐、口香糖9.8折，快来买啊！");
            } else {
                System.out.println("电影马上结束了，爆米花、可乐、口香糖9.8折，买回家吃吧！");
            }
        }
    }

    客户端：
    public class ProxyTest {
        public static void main(String[] args) {
            RealMovie realmovie = new RealMovie();
            Movie movie = new Cinema(realmovie);
            movie.play();
        }
	}	




动态代理：动态代理可以在程序运行期间根据需要动态的创建代理类及其实例来完成具体的功能
    
    接口：
    public interface Subject {
   		 public void doSomething();
	}

   实现（目标）类：
    public class RealSubject implements Subject {
        public void doSomething() {
            System.out.println("call doSomething()");
        }
	} 

	代理类：在动态代理中，代理类及其实例是程序自动生成的，因此我们不需要手动去创建代理类。在Java的动态代理机制中，InvocationHandler(Interface)接口和Proxy(Class)类是实现我们动态代理所必须用到的。事实上，Proxy通过使用InvocationHandler对象生成具体的代理代理对象，
        实现InvocationHandler接口，并且实现其invoke（）方法
        
       public class ProxyHandler implements InvocationHandler {

        private Object proxied;   // 被代理对象

        public ProxyHandler(Object proxied) {
            this.proxied = proxied;
        }

        public Object invoke(Object proxy, Method method, Object[] args)
                throws Throwable {

            // 在转调具体目标对象之前，可以执行一些功能处理
            System.out.println("前置增强处理： yoyoyo...");

            // 转调具体目标对象的方法(三要素：实例对象 + 实例方法 + 实例方法的参数)
            Object obj = method.invoke(proxied, args);
            //invoke()方法 第一个参数是代理对象 第二个是参数列表

            // 在转调具体目标对象之后，可以执行一些功能处理
            System.out.println("后置增强处理：hahaha...");

            return obj;
        }
}

客户端类：
    public class Test {
        public static void main(String args[]) {

            // 真实对象real
            Subject real = new RealSubject();

            // 生成real的代理对象
            //Proxy.newProxyInstance方法 第一个 类加载器  第二个 接口  第三个 代理类
            Subject proxySubject = (Subject) Proxy.newProxyInstance(
                    Subject.class.getClassLoader(), new Class[] { Subject.class },
                    new ProxyHandler(real));

            proxySubject.doSomething();
            System.out.println("代理对象的类型 ： " + proxySubject.getClass().getName());
            System.out.println("代理对象所在类的父类型 ： " + proxySubject.getClass().getGenericSuperclass());
        }
	}
我们还发现代理对象proxySubject所对应的类继承自java.lang.reflect.Proxy类，这也正是JDK动态代理机制无法实现对class的动态代理的原因：Java只允许单继承。

每个代理的实例都有一个与之关联的 InvocationHandler 实现类，如果代理的方法被调用，那么代理便会通知和转发给内部的 InvocationHandler 实现类，由它决定处理。
    public interface InvocationHandler {
        public Object invoke(Object proxy, Method method, Object[] args)
            throws Throwable;
	}
JDK通过 Proxy 的静态方法 newProxyInstance 动态地创建代理，该方法在Java中的声明如下：
    public static Object newProxyInstance(ClassLoader loader,
            Class<?>[] interfaces,
            InvocationHandler h)
    　事实上，Proxy 动态产生的代理对象调用目标方法时，代理对象会调用 InvocationHandler 实现类，所以 InvocationHandler 是实际执行者。

AOP机制是 Spring 所提供的核心功能之一，其既是Java动态代理机制的经典应用，也是动态AOP实现的代表。Spring AOP默认使用Java动态代理来创建AOP代理，具体通过以下几个步骤来完成：

    Spring IOC 容器创建Bean(目标类对象)；

    Bean创建完成后，Bean后处理器(BeanPostProcessor)根据具体的切面逻辑及Bean本身使用Java动态代理技术生成代理对象；

    应用程序使用上述生成的代理对象替代原对象来完成业务逻辑，从而达到增强处理的目的。
    
    
    
cglib代理：若目标类不存在接口，则使用cglib进行代理
    
  	CGLIB 代理的生成原理是 生成目标类的子类，而子类是增强过的，这个子类对象就是代理对象 。因此不能为final类型

	所以，使用CGLIB 生成动态代理， 要求目标类必须能够被继承，即不能是 final 的类 。 
    
    Cglib代理，也叫做子类代理。在内存中构建一个子类对象从而实现对目标对象功能的扩展。

CGLIB包的底层是通过使用一个小而快的字节码处理框架ASM，来转换字节码并生成新的类。不鼓励直接使用ASM，因为它要求你必须对JVM内部结构包括class文件的格式和指令集都很熟悉。
    目标对象：
    //目标对象
    public class UserDaoImpl {
        public void save() {
            System.out.println("保存用户信息进入数据库");
        }
    }
   代理类：
       public class ProxyFactory  implements MethodInterceptor{
        // 维护目标对象
        private Object target;

        public ProxyFactory(Object target) {
            this.target = target;
        }

        public ProxyFactory() {
        }

        // 给目标对象创建代理对象
        public Object getProxyInstance() {
            //1. 工具类
            Enhancer en = new Enhancer();
            //2. 设置父类
            en.setSuperclass(target.getClass());
            //3. 设置回调函数
            en.setCallback((MethodInterceptor) (o, method, args, methodProxy) -> {
                System.out.println("开始事务");
                //Object obj = method.invoke(target, args);
                Object obj = methodProxy.invokeSuper(o, args);
                System.out.println("提交事务");
                return obj;
            });
            //4. 创建子类(代理对象)
            return en.create();
        }
    }

代理类二：

    package com.sf.proxy.cglib;
    import java.lang.reflect.Method;
    import net.sf.cglib.proxy.Enhancer;
    import net.sf.cglib.proxy.MethodInterceptor;
    import net.sf.cglib.proxy.MethodProxy;
    public class ProxyMovie  implements MethodInterceptor{

        private RealMovie movie;

        public ProxyMovie(RealMovie movie) {
            super();
            this.movie = movie;
        }

        public ProxyMovie() {
            super();
            // TODO Auto-generated constructor stub
        }

        public Object getProxyInstance(){

              Enhancer en = new Enhancer();
              //设置子类
              en.setSuperclass(movie.getClass());
              //设置回调函数
              en.setCallback(this);
               //4. 创建子类(代理对象)
              return en.create();
        }

        @Override
        public Object intercept(Object obj, Method method, Object[] args,
                MethodProxy arg3) throws Throwable {
            // TODO Auto-generated method stub
            long start = System.currentTimeMillis();
            System.out.println("start time : " + start);
            method.invoke(movie, args);
            // 在后面做一些事情: 记录结束时间,并计算move()运行时间
            long end = System.currentTimeMillis();
            System.out.println("end time : " + end);
            System.out.println("spend all time : " + (end - start)/1000 + "s.");
            return null;
        }
    }

客户端类：
    public class Test {
        public static void main(String[] args) {
            ProxyFactory proxyFactory = new ProxyFactory(new UserDaoImpl());
            UserDaoImpl proxy = (UserDaoImpl) proxyFactory.getProxyInstance();
            proxy.save();
        }
	}

1、代理的类不能为final， 否则报错。
2、目标对象的方法如果为final/static, 那么就不会被拦截，即不会执行目标对象额外的业务方法。

另外：在Spring的AOP编程中，如果加入容器的目标对象有实现接口，用JDK代理；如果目标对象没有实现接口，用Cglib代理。
```



## 29.事务隔离级别



## 30.Java动态代理InvocationHandler

```java
每个代理的实例都有一个与之关联的 InvocationHandler 实现类，如果代理的方法被调用，那么代理便会通知和转发给内部的 InvocationHandler 实现类，由它决定处理。
    public interface InvocationHandler {
        public Object invoke(Object proxy, Method method, Object[] args)
            throws Throwable;
	}
```



## 31.拦截器之类的

## 32.Spring AOP DI

## 33.Hive

Hive**是基于**Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。

Hive利用HDFS存储数据，利用MapReduce查询数据

**Hive的本质是将 SQL 语句转换为 MapReduce 任务运行**

![img](https://img-blog.csdn.net/20180524104300145?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxMjEyeGlhbw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Driver 组件完成 HQL 查询语句从词法分析，语法分析，编译，优化，以及生成逻辑执行 计划的生成。生成的逻辑执行计划存储在 HDFS 中，并随后由 MapReduce 调用执行

　　Hive 的核心是驱动引擎， 驱动引擎由四部分组成：

　　　　(1) 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST）

　　　　(2) 编译器：编译器是将语法树编译为逻辑执行计划

　　　　(3) 优化器：优化器是对逻辑执行计划进行优化

　　　　(4) 执行器：执行器是调用底层的运行框架执行逻辑执行计划

group by:

```
group by操作表示按照某些字段的值进行分组，有相同的值放到一起，语法样例如下：
select col1,col2,count(1),sel_expr(聚合操作)
from tableName
where condition
group by col1,col2
having...
上述语句是从pv_users表中查询出性别和去重后的总人数，并且根据性别分组，然后将数据覆盖插入到pv_gender_sum中。
```

***\*注意：\****

***\*(1)：\****select后面的非聚合列必须出现在group by中(如上面的col1和col2)。

***\*(2)：\****除了普通列就是一些聚合操作。

在select语句中可以有多个聚合操作，但是如果***\*多个聚合操作中同时使用了distinct去重，那么distinct去重的列必须相同\****，如下语句不合法：

```
insert overwrite table pv_gender_agg
select pv_users.gender,count(distinct pv_users.userid),count(distinct pv_users.ip)
from pv_users
group by pv_users.gender;
上述语句之所以不合法，是因为distinct关键字去重的列不一样。一个是对userid去重，一个是对ip去重！
```

***\*select后面的非聚合列必须出现在group by中，否则非法\****，如下：

```
select uid,name,count(sal)
from users
group by uid;
上述语句是非法的因为select中出现了两个两个非聚合列即uid和name，但是group by中只有uid,所以非法。
```

count计数：

```
count(*)    所有值不全为NULL时，加1操作
count(1)    不管有没有值，只要有这条记录，值就加1
count(col)  col列里面的值为null，值不会加1，这个列里面的值不为NULL，才加1
```

order by:

```
order by后面可以有多列进行排序，默认按字典排序 
order by为全局排序 
order by需要reduce操作，且**只有一个reduce**，与配置无关。**数据量很大时，慎用。**
```

rank相关函数：

```
RANK() 排序相同时会重复，总数不会变
DENSE_RANK() 排序相同时会重复，总数会减少
ROW_NUMBER() 会根据顺序计算
通常与窗口函数一起使用
select emp_code,emp_name,row_number()  over(partition by emp_code order by area_rank asc) from ods_cotp.ods_cotp_tt_replica_five_star_award  
其中：over是窗口函数  根据什么列 按照 什么排列
```

窗口函数Over：https://blog.csdn.net/student__software/article/details/81636566

```
作用：
我们都知道在sql中有一类函数叫做聚合函数,例如sum()、avg()、max()等等,这类函数可以将多行数据按照规则聚集为一行,一般来讲聚集后的行数是要少于聚集前的行数的.但是有时我们想要既显示聚集前的数据,又要显示聚集后的数据,这时我们便引入了窗口函数.
**在深入研究Over字句之前，一定要注意：在SQL处理中，窗口函数都是最后一步执行，而且仅位于Order by字句之前.**
​	OVER()：指定分析函数工作的数据窗口大小，决定了聚合函数的范围，这个数据窗口大小可能会随着行的变而变化，同时可以使用以下进行限定范围。
​	Over子句之后第一个提到的就是Partition By.Partition 
By子句也可以称为查询分区子句，非常类似于Group By，都是将数据按照边界值分组，而Over之前的函数在每一个分组之内进行，如果超出了分组，则函数会重新计算.
```

```sql
例子：聚合函数+over

1.查询在2015年4月份购买过的顾客及总人数,我们便可以使用窗口函数去去实现
select name,count(*) over () from t_window where substring(orderdate,1,7) = '2015-04'

结果：
name    count_window_0
mart    5
mart    5
mart    5
mart    5
jack    5

2.只看去重后的结果的.针对于这种情况,我们有两种实现方式:  能使用group by代替distinc就不要使用distinct，
 	1.使用distinct:
 	select distinct name,count(*) over () from t_window where substring(orderdate,1,7) = '2015-04'
 	2.使用group by
 	select name,count(*) over ()from t_window where substring(orderdate,1,7) = '2015-04' group by name
 	
结果：
name count_window_0 
mart 2 
jack 2
```
```sql
例子：partition by子句
Over子句之后第一个提到的就是Partition By.Partition By子句也可以称为查询分区子句，非常类似于Group By，都是将数据按照边界值分组，而Over之前的函数在每一个分组之内进行，如果超出了分组，则函数会重新计算.

1.我们想要去看顾客的购买明细及月购买总额,可以执行如下的sql: （数据按照月进行汇总）
select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from t_window

结果：
name    orderdate   cost    sum_window_0
jack    2015-01-01  10  205
jack    2015-01-08  55  205
tony    2015-01-07  50  205
jack    2015-01-05  46  205
tony    2015-01-04  29  205
tony    2015-01-02  15  205
jack    2015-02-03  23  23
mart    2015-04-13  94  341
jack    2015-04-06  42  341
mart    2015-04-11  75  341
mart    2015-04-09  68  341
mart    2015-04-08  62  341
neil    2015-05-10  12  12
neil    2015-06-12  80  80。

select *,row_number() over(partition by area_code order by k_finger_income desc)  from dm_user_profile.t_coupon_dtl ; 

数据：
775A 2
775A 3
775AB 5
775AB 6

结果：

775A 2     1
775A 3     2
775AB 5    1
775AB 6    2

下面语句的row_number不会生效：原因是where语句在后面
select *,row_number() over(partition by area_code,dept_code order by k_finger_income desc)  from dm_user_profile.t_coupon_dtl where insert_flag = 2 ; 

找点部所在区域点部的排名：
select area_code,dept_code, income_ct ,ROW_NUMBER() over(partition by area_code order by income_ct desc) from (
select area_code,dept_code,sum(k_finger_income) as income_ct  from dm_user_profile.t_coupon_dtl where insert_flag=2 group by area_code,dept_code) t 


```

```
3.order by子句
order by子句会让输入的数据强制排序（文章前面提到过，窗口函数是SQL语句最后执行的函数，因此可以把SQL结果集想象成输入数据）。Order By子句对于诸如Row_Number()，Lead()，LAG()等函数是必须的，因为如果数据无序，这些函数的结果就没有任何意义。因此如果有了Order By子句，则Count()，Min()等计算出来的结果就没有任何意义。

例子：选取每个月话费明细+总额度+按照订单日期进行排序
select name,orderdate,cost,sum(cost) over(partition by month(orderdate) order by orderdate )
from t_window

结果：
name    orderdate   cost    sum_window_0
jack    2015-01-01  10  10
tony    2015-01-02  15  25
tony    2015-01-04  29  54
jack    2015-01-05  46  100
tony    2015-01-07  50  150
jack    2015-01-08  55  205
jack    2015-02-03  23  23
jack    2015-04-06  42  42
mart    2015-04-08  62  104
mart    2015-04-09  68  172
mart    2015-04-11  75  247
mart    2015-04-13  94  341
neil    2015-05-10  12  12
neil    2015-06-12  80  80

```

```

语句：
Insert overwrite table dm_prod_sasp.t_coupon_dtl partition(inc_day=	$[time(yyyyMMdd,-1d)]) 
--先查找满足的数据 然后在对其进行排序
select key_id,area_name,d.area_code,division_name,division_code,dept_name,d.dept_code,emp_name,emp_code,emp_phone,insert_flag,emp_total_num,k_finger_coupon_used_num,k_finger_coupon_used_rate,k_finger_discount,k_finger_income,k_finger_income_l,k_finger_cust_num,k_finger_cust_mom,k_finger_emp_num,k_finger_consignor_emp_num,k_finger_consignee_emp_num,k_finger_consignor_cust_num,k_finger_consignor_coupon_num,k_finger_consignor_coupon_used_num,k_finger_consignor_coupon_used_rate,k_finger_consignor_income,k_finger_consignor_discount,k_finger_consignee_cust_num,k_finger_consignee_coupon_num,k_finger_consignee_coupon_used_num,k_finger_consignee_coupon_used_rate,k_finger_consignee_income,k_finger_consignee_discount,store_card_c_num,store_card_c_num_mom,store_card_c_emp_num,store_card_c_deposit_cust_num,store_card_c_deposit_amount,store_card_c_consume_cust_num,store_card_c_consume_amount,store_card_c_consume_amount_l,store_card_a_num,store_card_a_num_mom,store_card_a_emp_num,store_card_a_deposit_cust_num,store_card_a_deposit_amount,store_card_a_consume_cust_num,store_card_a_consume_amount,store_card_a_consume_amount_l,svip_cust_num,svip_cust_num_mom,svip_emp_num,svip_amount,svip_amount_l,svip_consignor_income,svip_consignor_num,vfriend_emp_num as v_friend_emp_num,vfriend_cust_num,vfriend_cust_num_mom,vfriend_consignor_income,vfriend_consignor_income_l,vfriend_consignor_num,yuejie_coupon_used_num,yuejie_coupon_used_num_mom,yuejie_coupon_emp_num,yuejie_coupon_used_rate,yuejie_coupon_used_rate_l,is_valid,k_finger_rank,store_card_c_rank,store_card_a_rank,svip_rank,vfriend_rank,yuejie_rank,division_manager_phone,dept_manager_phone
 from dm_user_profile.t_coupon_dtl d inner join (
select 
area_code,dept_code
,rank() over(partition by area_code order by k_finger_income_ct desc) k_finger_rank
,rank() over(partition by area_code order by store_card_c_consume_amount_ct desc) store_card_c_rank 
,rank() over(partition by area_code order by store_card_a_consume_amount_ct desc) store_card_a_rank 
,rank() over(partition by area_code order by svip_amount_ct desc) svip_rank 
,rank() over(partition by area_code order by vfriend_consignor_income_ct desc) vfriend_rank 
,rank() over(partition by area_code order by yuejie_coupon_used_rate_ct desc) yuejie_rank 
from (select 
area_code,dept_code,
sum(svip_amount) as k_finger_income_ct,
sum(store_card_c_consume_amount) as store_card_c_consume_amount_ct  ,
sum(store_card_a_consume_amount) as store_card_a_consume_amount_ct , 
sum(svip_amount) as svip_amount_ct,
sum(vfriend_consignor_income) as vfriend_consignor_income_ct,
sum(yuejie_coupon_used_rate) as yuejie_coupon_used_rate_ct
from dm_user_profile.t_coupon_dtl where insert_flag=2 and inc_day = date_format(DATE_SUB(FROM_UNIXTIME(UNIX_TIMESTAMP()),1),'yyyyMMdd') group by 
area_code,dept_code) t) t1
ON d.area_code = t1.area_code and d.dept_code = t1.dept_code and d.insert_flag = 2

union

select key_id,area_name,d.area_code,division_name,d.division_code,dept_name,d.dept_code,emp_name,emp_code,emp_phone,insert_flag,emp_total_num,k_finger_coupon_used_num,k_finger_coupon_used_rate,k_finger_discount,k_finger_income,k_finger_income_l,k_finger_cust_num,k_finger_cust_mom,k_finger_emp_num,k_finger_consignor_emp_num,k_finger_consignee_emp_num,k_finger_consignor_cust_num,k_finger_consignor_coupon_num,k_finger_consignor_coupon_used_num,k_finger_consignor_coupon_used_rate,k_finger_consignor_income,k_finger_consignor_discount,k_finger_consignee_cust_num,k_finger_consignee_coupon_num,k_finger_consignee_coupon_used_num,k_finger_consignee_coupon_used_rate,k_finger_consignee_income,k_finger_consignee_discount,store_card_c_num,store_card_c_num_mom,store_card_c_emp_num,store_card_c_deposit_cust_num,store_card_c_deposit_amount,store_card_c_consume_cust_num,store_card_c_consume_amount,store_card_c_consume_amount_l,store_card_a_num,store_card_a_num_mom,store_card_a_emp_num,store_card_a_deposit_cust_num,store_card_a_deposit_amount,store_card_a_consume_cust_num,store_card_a_consume_amount,store_card_a_consume_amount_l,svip_cust_num,svip_cust_num_mom,svip_emp_num,svip_amount,svip_amount_l,svip_consignor_income,svip_consignor_num,vfriend_emp_num as v_friend_emp_num,vfriend_cust_num,vfriend_cust_num_mom,vfriend_consignor_income,vfriend_consignor_income_l,vfriend_consignor_num,yuejie_coupon_used_num,yuejie_coupon_used_num_mom,yuejie_coupon_emp_num,yuejie_coupon_used_rate,yuejie_coupon_used_rate_l,is_valid,k_finger_rank,store_card_c_rank,store_card_a_rank,svip_rank,vfriend_rank,yuejie_rank,division_manager_phone,dept_manager_phone
 from dm_user_profile.t_coupon_dtl d inner join (
select 
area_code,division_code
,rank() over(partition by area_code order by k_finger_income_ct desc) k_finger_rank
,rank() over(partition by area_code order by store_card_c_consume_amount_ct desc) store_card_c_rank 
,rank() over(partition by area_code order by store_card_a_consume_amount_ct desc) store_card_a_rank 
,rank() over(partition by area_code order by svip_amount_ct desc) svip_rank 
,rank() over(partition by area_code order by vfriend_consignor_income_ct desc) vfriend_rank 
,rank() over(partition by area_code order by yuejie_coupon_used_rate_ct desc) yuejie_rank 
from (select 
area_code,division_code,
sum(svip_amount) as k_finger_income_ct,
sum(store_card_c_consume_amount) as store_card_c_consume_amount_ct  ,
sum(store_card_a_consume_amount) as store_card_a_consume_amount_ct , 
sum(svip_amount) as svip_amount_ct,
sum(vfriend_consignor_income) as vfriend_consignor_income_ct,
sum(yuejie_coupon_used_rate) as yuejie_coupon_used_rate_ct
from dm_user_profile.t_coupon_dtl where insert_flag=1 and inc_day = date_format(DATE_SUB(FROM_UNIXTIME(UNIX_TIMESTAMP()),1),'yyyyMMdd') group by 
area_code,division_code) t) t1
ON d.area_code = t1.area_code and d.division_code = t1.division_code and d.insert_flag = 1

union

select key_id,area_name,d.area_code,division_name,division_code,dept_name,d.dept_code,emp_name,d.emp_code,emp_phone,insert_flag,emp_total_num,k_finger_coupon_used_num,k_finger_coupon_used_rate,k_finger_discount,k_finger_income,k_finger_income_l,k_finger_cust_num,k_finger_cust_mom,k_finger_emp_num,k_finger_consignor_emp_num,k_finger_consignee_emp_num,k_finger_consignor_cust_num,k_finger_consignor_coupon_num,k_finger_consignor_coupon_used_num,k_finger_consignor_coupon_used_rate,k_finger_consignor_income,k_finger_consignor_discount,k_finger_consignee_cust_num,k_finger_consignee_coupon_num,k_finger_consignee_coupon_used_num,k_finger_consignee_coupon_used_rate,k_finger_consignee_income,k_finger_consignee_discount,store_card_c_num,store_card_c_num_mom,store_card_c_emp_num,store_card_c_deposit_cust_num,store_card_c_deposit_amount,store_card_c_consume_cust_num,store_card_c_consume_amount,store_card_c_consume_amount_l,store_card_a_num,store_card_a_num_mom,store_card_a_emp_num,store_card_a_deposit_cust_num,store_card_a_deposit_amount,store_card_a_consume_cust_num,store_card_a_consume_amount,store_card_a_consume_amount_l,svip_cust_num,svip_cust_num_mom,svip_emp_num,svip_amount,svip_amount_l,svip_consignor_income,svip_consignor_num,vfriend_emp_num as v_friend_emp_num,vfriend_cust_num,vfriend_cust_num_mom,vfriend_consignor_income,vfriend_consignor_income_l,vfriend_consignor_num,yuejie_coupon_used_num,yuejie_coupon_used_num_mom,yuejie_coupon_emp_num,yuejie_coupon_used_rate,yuejie_coupon_used_rate_l,is_valid,k_finger_rank,store_card_c_rank,store_card_a_rank,svip_rank,vfriend_rank,yuejie_rank,division_manager_phone,dept_manager_phone
 from dm_user_profile.t_coupon_dtl d inner join (
select 
area_code,emp_code
,rank() over(partition by area_code order by k_finger_income_ct desc) k_finger_rank
,rank() over(partition by area_code order by store_card_c_consume_amount_ct desc) store_card_c_rank 
,rank() over(partition by area_code order by store_card_a_consume_amount_ct desc) store_card_a_rank 
,rank() over(partition by area_code order by svip_amount_ct desc) svip_rank 
,rank() over(partition by area_code order by vfriend_consignor_income_ct desc) vfriend_rank 
,rank() over(partition by area_code order by yuejie_coupon_used_rate_ct desc) yuejie_rank 
from (select 
area_code,emp_code,
sum(svip_amount) as k_finger_income_ct,
sum(store_card_c_consume_amount) as store_card_c_consume_amount_ct  ,
sum(store_card_a_consume_amount) as store_card_a_consume_amount_ct , 
sum(svip_amount) as svip_amount_ct,
sum(vfriend_consignor_income) as vfriend_consignor_income_ct,
sum(yuejie_coupon_used_rate) as yuejie_coupon_used_rate_ct
from dm_user_profile.t_coupon_dtl where insert_flag=3 and inc_day = date_format(DATE_SUB(FROM_UNIXTIME(UNIX_TIMESTAMP()),1),'yyyyMMdd') group by 
area_code,emp_code) t) t1
ON d.area_code = t1.area_code and d.emp_code = t1.emp_code and d.insert_flag = 3


```



## 34.hystrix

## 35.线程

## ***36.线程池***

## 37.枚举

##  38.HDFS 

## 39.mapReduces

## 40.Quartz

```
1.Job-任务：一个接口，只有一个execute方法，使用时该方法内容即为需要执行的任务逻辑，还有个关联的JobDetail接口，注意这两者并不是继承关系，Quartz在每次执行Job时，都重新创建一个Job实例，所以它不直接接受一个Job的实例，相反它接收一个Job实现类，以便运行时通过newInstance()的反射机制实例化Job。因此需要通过一个类来描述Job的实现类及其它相关的静态信息，如Job名字、描述、关联监听器等信息，JobDetail承担了这一角色，Quartz源码中描述两者关系:"Quartz不会保存一个Job接口的实例，但可以通过JobDetail来定义一个实例"，JobDetail包含一个getJobClass()获得Job实例字节码的方法；
2.Trigger-触发器：手枪的扳机，什么时候发射，就看什么时候触发了该类设定的条件，可自由定义触发规则，多个触发器可作用于一个Job，但一个触发器只可作用于一个Job；
3.Scheduler-调度器：代表一个Quartz的独立运行容器，Trigger和JobDetail可以注册到Scheduler中，两者在Scheduler中拥有各自的组及名称，组及名称是Scheduler查找定位容器中某一对象的依据，Trigger的组及名称必须唯一，JobDetail的组和名称也必须唯一（但可以和Trigger的组和名称相同，因为它们是不同类型的）；
例子1：
1.实现接口：
public class HelloJob implements Job {
    @Override
    public void execute(JobExecutionContext context) throws JobExecutionException {
        System.out.println(System.currentTimeMillis()+"helloWorld");
    }
}
2.主方法：
public class ScheduledTaskMain {
    public static void main(String[] args) throws SchedulerException {
        /*创建一个jobDetail的实例，将该实例与HelloJob Class绑定*/
        JobDetail jobDetail = JobBuilder.newJob(HelloJob.class).withIdentity("HelloJob").build();
        /*创建一个触发器，每2秒执行一次任务，一直持续下去*/
        SimpleTrigger cronTrigger=  TriggerBuilder.newTrigger().withIdentity("HelloTrigger").startNow()
                .withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInSeconds(2).repeatForever()).build();
        /*创建schedule实例*/
        StdSchedulerFactory factory = new StdSchedulerFactory();
        Scheduler scheduler = factory.getScheduler();
        /*将Job和trigger放入Scheduler容器*/
        scheduler.scheduleJob(jobDetail,cronTrigger);
        scheduler.start();
    }
}
```

时间配置：

CronTrigger配置完整格式为： [秒] [分] [小时] [日] [月] [周] [年]

| 序号 | 说明 | 是否必填 | 允许填写的值       | 允许的通配符  |
| ---- | ---- | -------- | ------------------ | ------------- |
| 1    | 秒   | 是       | 0-59               | , - * /       |
| 2    | 分   | 是       | 0-59               | , - * /       |
| 3    | 小时 | 是       | 0-23               | , - * /       |
| 4    | 日   | 是       | 1-31               | , - * ? / L W |
| 5    | 月   | 是       | 1-12 or JAN-DEC    | , - * /       |
| 6    | 周   | 是       | 1-7 or SUN-SAT     | , - * ? / L # |
| 7    | 年   | 否       | empty 或 1970-2099 | , - * /       |

```
*：表示所有值 设置在分上 则表示每一分钟
？：表示不指定值 不需要关心周几 则在这个周的位置上设置？
-:表示区间 在小时上设置 "10-12",表示 10,11,12点都会触发。
,:表示多个值  例如在周字段上设置 "MON,WED,FRI" 表示周一，周三和周五触发
/：用于递增触发 秒上面设置"5/15" 表示从5秒开始，每增15秒触发(5,20,35,50)。 在月字段上设置'1/3'所示每月1号开始，每隔三天触发一次
L：表示最后的意思 在日字段设置上，表示当月的最后一天(依据当前月份，如果是二月还会依据是否是润年[leap]), 在周字段上表示星期六，相当于"7"或"SAT"。如果在"L"前加上数字，则表示该数据的最后一个。例如在周字段上设置"6L"这样的格式,则表示“本月最后一个星期五" 
W:表示离指定日期最近的工作日（周一到周五）  例如在日字段上设置"15W"，表示离每月15号最近的那个工作日触发。如果15号正好是周六，则找最近的周五(14号)触发, 如果15号是周未，则找最近的下周一(16号)触发.如果15号正好在工作日(周一至周五)，则就在该天触发。如果指定格式为 "1W",它则表示每月1号往后最近的工作日触发。如果1号正是周六，则将在3号下周一触发。(注，"W"前只能设置具体的数字,不允许区间"-").
#：序号，表示表示每月的第几个周几，例如在周字段上设置"6#3"表示在每月的第三个周六.注意如果指定"#5",正好第五周没有周六，则不会触发该配置(用在母亲节和父亲节再合适不过了) ；
'L'和 'W'可以一组合使用。如果在日字段上设置"LW",则表示在本月的最后一个工作日触发；
周字段的设置，若使用英文字母是不区分大小写的，即MON 与mon相同；

```

| `0 0 12 * * ?`             | 每天12点触发                                                 |
| -------------------------- | ------------------------------------------------------------ |
| `0 15 10 ? * *`            | 每天10点15分触发                                             |
| `0 15 10 * * ?`            | 每天10点15分触发                                             |
| `0 15 10 * * ? *`          | 每天10点15分触发                                             |
| `0 15 10 * * ? 2005`       | 2005年每天10点15分触发                                       |
| `0 * 14 * * ?`             | 每天下午的 2点到2点59分每分触发                              |
| `0 0/5 14 * * ?`           | 每天下午的 2点到2点59分(整点开始，每隔5分触发)               |
| `0 0/5 14,18 * * ?`        | 每天下午的 2点到2点59分、18点到18点59分(整点开始，每隔5分触发) |
| `0 0-5 14 * * ?`           | 每天下午的 2点到2点05分每分触发                              |
| `0 10,44 14 ? 3 WED`       | 3月分每周三下午的 2点10分和2点44分触发                       |
| `0 15 10 ? * MON-FRI`      | 从周一到周五每天上午的10点15分触发                           |
| `0 15 10 15 * ?`           | 每月15号上午10点15分触发                                     |
| `0 15 10 L * ?`            | 每月最后一天的10点15分触发                                   |
| `0 15 10 ? * 6L`           | 每月最后一周的星期五的10点15分触发                           |
| `0 15 10 ? * 6L 2002-2005` | 从2002年到2005年每月最后一周的星期五的10点15分触发           |
| `0 15 10 ? * 6#3`          | 每月的第三周的星期五开始触发                                 |
| `0 0 12 1/5 * ?`           | 每月的第一个中午开始每隔5天触发一次                          |
| `0 11 11 11 11 ?`          | 每年的11月11号 11点11分触发(光棍节)                          |



## 41.开闭原则

软件实体应当对扩展开放，对修改关闭（Software entities should be open for extension，but closed for modification），这就是开闭原则的经典定义。

开闭原则的含义是：当应用的需求改变时，在不修改软件实体的源代码或者二进制代码的前提下，可以扩展模块的功能，使其满足新的需求。

### 开闭原则的作用

开闭原则是面向对象程序设计的终极目标，它使软件实体拥有一定的适应性和灵活性的同时具备稳定性和延续性。具体来说，其作用如下。

#### 1. 对软件测试的影响

软件遵守开闭原则的话，软件测试时只需要对扩展的代码进行测试就可以了，因为原有的测试代码仍然能够正常运行。

#### 2. 可以提高代码的可复用性

粒度越小，被复用的可能性就越大；在面向对象的程序设计中，根据原子和抽象编程可以提高代码的可复用性。

#### 3. 可以提高软件的可维护性

遵守开闭原则的软件，其稳定性高和延续性强，从而易于扩展和维护。

### 开闭原则的实现方法

可以通过“抽象约束、封装变化”来实现开闭原则，即通过接口或者抽象类为软件实体定义一个相对稳定的抽象层，而将相同的可变因素封装在相同的具体实现类中。

因为抽象灵活性好，适应性广，只要抽象的合理，可以基本保持软件架构的稳定。而软件中易变的细节可以从抽象派生来的实现类来进行扩展，当软件需要发生变化时，只需要根据需求重新派生一个实现类来扩展就可以了。



## 10.10进行记录

## 42.String.StringBuffer.StringBuilder区别：

```
blog:https://blog.csdn.net/Farrell_zeng/article/details/100153345

string:固定长度
StirngBuffer:线程安全
StringBuilder:线程不安全
```

## 43:StringUtils常用方法

```

isNoneBlank与isNotBlank区别:
参数不一样，isNotBlank只支持单个参数，而isNoneBlank可以支持多个参数，甚至String数组，用来判断数组里的每一个字符串都是isNotBlank的。
```

## 44.为什么不推荐使用runtimeException

```
1.方便捕获处理 个性化异常抛出
    try {
        // ...逻辑
    } catch(RuntimeException e) {
        if("user not found".equals(e.getMessage())) {
            // ...逻辑
        } else if("password not match".equals(e.getMessage())) {
            // ...逻辑
        }
    }
    逻辑太乱了

    try {
        // ...逻辑
    } catch(UserNotFoundException e) {
        // ...逻辑
    } catch(PasswordNotMatchException e) {
        // ...逻辑
    }
2.
```



个性化异常抛出？

## 45.集合框架

### List

### Set

set的复制：

```
Set<String> desSet = new HashSet<>(sourceSet);
```

### Map：

```
四个函数的区别：

1 putIfAbsent
default V putIfAbsent(K key,V value)
If the specified key is not already associated with a value (or is mapped to null) associates it with the given value and returns null, else returns the current value
上面是官方的解释，意思是如果给定的key不存在（或者key对应的value为null），关联给定的key和给定的value，并返回null；如果存在，返回当前值（不会把value放进去）

2 computeIfAbsent


 
map.computeIfAbsent(key, k -> new Value(f(k)));
map.computeIfAbsent(key, k -> new HashSet<V>()).add(v);

3 computeIfPresent

4 compute




putIfAbsent和computeIfAbsent

都是在key不存在的时候才会建立key和value的映射关系；
putIfAbset不论传入的value是否为空，都会建立映射（并不适合所有子类，例如HashTable），而computeIfAbsent方法，当存入value为空时，不做任何操作
当key不存在时，返回的都是新的value（为什么不说新插入的value），即使computeIfAbsent在传入的value为null时，不会新建映射关系，但返回的也是null；     
     computeIfPresent和computeIfAbsent

这两个方法正好相反，前者是在key存在时，才会用新的value替换oldValue
当传入的key存在，并且传入的value为null时，前者会remove（key），把传入的key对应的映射关系移除；而后者不论何时都不会remove()；
前者只有在key存在，并且传入的value不为空的时候，返回值是value，其他情况都是返回null；后者只有在key不存在，并且传入的value不为null的时候才会返回value，其他情况都返回null；
     compute

新传入的value不为null就建立映射关系（也就是说不论key是否为null，具体子类再具体分析）
新传入的value为null时：key已存在，且老的对应value不为null，移除改映射关系，返回null；否则，直接返回null

map.computeIfAbsent("a", k -> new HashSet<>()).add("1");
其中k->new HashSet<>() 表示在key值为a的时候，没有映射 value的时候，new一个HashSet并将其与 key进行映射，类似代码：
        if (set == null) {
            set = new HashSet<>();
            map.put(key, set);
        }


computeIfAbsent：map.computeIfAbsent("a", k -> new HashSet<>()).add("1");
当a对应的value不存在应该干嘛
computeIfPresent： 
map.computeIfPresent("a", (k, set) -> {
    set.remove("2");
    return set;
});
当a对应的value存在应该干嘛 
```



### 1.map访问方式

```
map.values():获取map的values值
map.keyset():获取map的key值
map.entrySet():获取对应的关系
1.通过Map.keySet()遍历key
     //第一种：普遍使用，二次取值
      System.out.println("通过Map.keySet遍历key和value：");
      for (String key : map.keySet()) {
       System.out.println("key= "+ key + " and value= " + map.get(key));
      }
2.通过Map.entrySet使用iterator遍历key和value： 拿取关系 使用迭代器
     System.out.println("通过Map.entrySet使用iterator遍历key和value：");
      Iterator<Map.Entry<String, String>> it = map.entrySet().iterator();
      while (it.hasNext()) {
       Map.Entry<String, String> entry = it.next();
       System.out.println("key= " + entry.getKey() + " and value= " + entry.getValue());
      }
3.通过Map.entrySet遍历key和value 常用  拿取关系
      System.out.println("通过Map.entrySet遍历key和value");
      for (Map.Entry<String, String> entry : map.entrySet()) {
       System.out.println("key= " + entry.getKey() + " and value= " + entry.getValue());
      }
4.遍历value1
     System.out.println("通过Map.values()遍历所有的value，但不能遍历key");
      for (String v : map.values()) {
       System.out.println("value= " + v);
      }
```



### 其中求交集 并集 差集的方法：

```
Sets.intersection（set1,set2,set3） 求多个集合的交集
```

```
Collectors.groupingBy（）：Collectors.groupingBy根据一个或多个属性对集合中的项目进行分组
```

## 46.MYSQL中TRUNCATE和DELETE的区别

https://www.jianshu.com/p/ddc5b65e63af

![img](https://upload-images.jianshu.io/upload_images/1849270-e7572fa6e1aac935.png?imageMogr2/auto-orient/strip|imageView2/2/w/388/format/webp)

打开、关闭、查看mysql的外键约束命令



```
禁用外键约束
SET FOREIGN_KEY_CHECKS=0;

启动外键约束
SET FOREIGN_KEY_CHECKS=1;

查看当前FOREIGN_KEY_CHECKS的值可用如下命令
SELECT @@FOREIGN_KEY_CHECKS;


条件删除：
    truncate ：不能条件删除 ，删除所有数据 truncate table name
    delete：可以条件删除
事务回滚：、
	truncate ：不支持 因为不写日志 DDL
    delete：支持 写日志 dml
清理速度：
	truncate ：不写日志 使用资源少 但是不安全
    delete：写日志 使用资源多 安全
 高水位重置：
 	truncate ：高水位重置
    delete：不重置高水位 全表查找会查找删除的空间
```

## 47.Spring AOP IOC

### Spring:

```
1.使用xml注入bean：
	<!-- 注入bean-->
	<bean id="people" class="com.pojo.People">
		属性注入
		<property name="name" value="李四"/>
	</bean>

取值：
	ApplicationContext xml=new ClassPathXmlApplicationContext("spring.xml");
    People people=(People)xml.getBean("people");
    people.sayHello();

2.使用注解注入bean:
	<context:component-scan base-package="com.pojo"/>//扫描包
	实例化Bean有四个注解
        @Component
        @Service：业务层
        @Controller：WEB层
        @Repository：持久层
    @Component：使用注解注入bean
    @value:赋值
    
    使用applcaitopnContext()
    
选取一个即可，扫描包+注解 或者 xml<bean>的配置 不然会注入两次


```



AOP:拦截器链

如何保证前置 方法 后置的顺序  ？

advisor

增强方法：

拦截器：intercept

jdk代理：目标对象必须实现接口 使用invocationHadle 反射 invoke方法进行代理

cglib代理：通过ASM技术创建子类 使用拦截器进行代理

```
 @Before:前置通知，在方法执行之前返回 
 @After:后置通知，在方法执行后执行 
 @AfterRunning:返回通知，在方法返回结果之后执行 
 @AfterThrowing:异常通知，在方法抛出异常之后 
 @Around:环绕通知，围绕着方法执行
```



## 48.布隆过滤器：

```
直观的说，bloom算法类似一个hash set，用来判断某个元素（key）是否在某个集合中。
和一般的hash set不同的是，这个算法无需存储key的值，对于每个key，只需要k个比特位，每个存储一个标志，用来判断key是否在集合中。

算法：
1. 首先需要k个hash函数，每个函数可以把key散列成为1个整数
2. 初始化时，需要一个长度为n比特的数组，每个比特位初始化为0
3. 某个key加入集合时，用k个hash函数计算出k个散列值，并把数组中对应的比特位置为1
4. 判断某个key是否在集合时，用k个hash函数计算出k个散列值，并查询数组中对应的比特位，如果所有的比特位都是1，认为在集合中。

优点：不需要存储key，节省空间

缺点：
1. 算法判断key在集合中时，有一定的概率key其实不在集合中
2. 无法删除

如果想判断一个元素是不是在一个集合里，一般想到的是将所有元素保存起来，然后通过比较确定。链表，树等等数据结构都是这种思路. 但是随着集合中元素的增加，我们需要的存储空间越来越大，检索速度也越来越慢。不过世界上还有一种叫作散列表（又叫哈希表，Hash table）的数据结构。它可以通过一个Hash函数将一个元素映射成一个位阵列（Bit Array）中的一个点。这样一来，我们只要看看这个点是不是 1 就知道可以集合中有没有它了。这就是布隆过滤器的基本思想。

Hash面临的问题就是冲突。假设 Hash 函数是良好的，如果我们的位阵列长度为 m 个点，那么如果我们想将冲突率降低到例如 1%, 这个散列表就只能容纳 m/100 个元素。显然这就不叫空间有效了（Space-efficient）。解决方法也简单，就是使用多个 Hash，如果它们有一个说元素不在集合中，那肯定就不在。如果它们都说在，虽然也有一定可能性它们在说谎，不过直觉上判断这种事情的概率是比较低的。

布隆过滤器其中重要的实现就是位图的实现，也就是位数组，并且在这个数组中每一个位置只占有1个bit，而每个bit只有0和1两种状态。如上图bitarray所示！bitarray也叫bitmap，大小也就是布隆过滤器的大小。

假设一种有k个哈希函数，且每个哈希函数的输出范围都大于m，接着将输出值对k取余（%m）,就会得到k个[0, m-1]的值，由于每个哈希函数之间相互独立，因此这k个数也相互独立，最后将这k个数对应到bitarray上并标记为1（涂黑）。

等判断时，将输入对象经过这k个哈希函数计算得到k个值，然后判断对应bitarray的k个位置是否都为1（是否标黑），如果有一个不为黑，那么这个输入对象则不在这个集合中，也就不是黑名单了！如果都是黑，那说明在集合中，但有可能会误，由于当输入对象过多，而集合也就是bitarray过小，则会出现大部分为黑的情况，那样就容易发生误判！因此使用布隆过滤器是需要容忍错误率的，即使很低很低！

假设输入对象个数为n，bitarray大小（也就是布隆过滤器大小）为m，所容忍的误判率p和哈希函数的个数k。计算公式如下：（小数向上取整）
```

![img](https://pic1.zhimg.com/80/v2-fc1fb96508a363b17d1bb7737dc51e54_720w.jpg)

## 49.自动装箱与拆箱：

```
//自动装箱
Integer total = 99;
//自动拆箱
int totalprim = total;

自动装箱：自动将基本数据类型转换为包装器类型
自动拆箱：自动将包装器类型转换为基本数据类型

Interger.vauleOf():基本转包装   变量.intValue():将包装转为基本
Integer total = 99; 
执行上面那句代码的时候，系统为我们执行了： 
Integer total = Integer.valueOf(99);

int totalprim = total; 
执行上面那句代码的时候，系统为我们执行了： 
int totalprim = total.intValue();

```

![这里写图片描述](http://img.blog.csdn.net/20150922151443893)

## 50：ThreadLocal

**threadlocal**而是一个线程内部的存储类，可以在指定线程内存储数据，数据存储以后，只有指定线程可以得到存储数据。

大致意思就是ThreadLocal提供了线程内存储变量的能力，这些变量不同之处在于每一个线程读取的变量是对应的互相独立的。通过get和set方法就可以得到当前线程对应的值。

做个不恰当的比喻，从表面上看ThreadLocal相当于维护了一个map，key就是当前的线程，value**就是需要存储的对象**。

**这里的这个比喻是不恰当的，实际上是ThreadLocal的静态内部类ThreadLocalMap为每个Thread都维护了一个数组table，ThreadLocal确定了一个数组下标，而这个下标就是value存储的对应位置。**



```java
//set 方法
public void set(T value) {
      //获取当前线程
      Thread t = Thread.currentThread();
      //实际存储的数据结构类型
      ThreadLocalMap map = getMap(t);
      //如果存在map就直接set，没有则创建map并set
      if (map != null)
          map.set(this, value);
      else
          createMap(t, value);
  }
  
//getMap方法
ThreadLocalMap getMap(Thread t) {
      //thred中维护了一个ThreadLocalMap
      return t.threadLocals;
 }
 
//createMap
void createMap(Thread t, T firstValue) {
      //实例化一个新的ThreadLocalMap，并赋值给线程的成员变量threadLocals
      t.threadLocals = new ThreadLocalMap(this, firstValue);
}

/* ThreadLocal values pertaining to this thread. This map is maintained
     * by the ThreadLocal class. */
    ThreadLocal.ThreadLocalMap threadLocals = null;
```

从上面代码可以看出**每个线程持有一个ThreadLocalMap对象**。每一个新的线程Thread都会实例化一个ThreadLocalMap并赋值给成员变量threadLocals，使用时若已经存在threadLocals则直接使用已经存在的对象。

```java
//Entry为ThreadLocalMap静态内部类，对ThreadLocal的若引用
//同时让ThreadLocal和储值形成key-value的关系
static class Entry extends WeakReference<ThreadLocal<?>> {
    /** The value associated with this ThreadLocal. */
    Object value;

    Entry(ThreadLocal<?> k, Object v) {
           super(k);
            value = v;
    }
}

//ThreadLocalMap构造方法
ThreadLocalMap(ThreadLocal<?> firstKey, Object firstValue) {
        //内部成员数组，INITIAL_CAPACITY值为16的常量
        table = new Entry[INITIAL_CAPACITY];
        //位运算，结果与取模相同，计算出需要存放的位置
        //threadLocalHashCode比较有趣
        int i = firstKey.threadLocalHashCode & (INITIAL_CAPACITY - 1);
        table[i] = new Entry(firstKey, firstValue);
        size = 1;
        setThreshold(INITIAL_CAPACITY);
}

每个Thread都存在一个ThreadLocal,然后每个ThreadLocal 中都存在一个ThreadLocalMap,该Map存在table Entry,然后对ThreadLocal进行存储。其中table就是Entry
```

```java
//在某一线程声明了ABC三种类型的ThreadLocal
ThreadLocal<A> sThreadLocalA = new ThreadLocal<A>();
ThreadLocal<B> sThreadLocalB = new ThreadLocal<B>();
ThreadLocal<C> sThreadLocalC = new ThreadLocal<C>();

由前面我们知道对于一个Thread来说只有持有一个ThreadLocalMap，所以ABC对应同一个ThreadLocalMap对象。为了管理ABC，于是将他们存储在一个数组的不同位置，而这个数组就是上面提到的Entry型的数组table。

那么问题来了，ABC在table中的位置是如何确定的？为了能正常够正常的访问对应的值，肯定存在一种方法计算出确定的索引值i，show me code。

      //ThreadLocalMap中set方法。
  private void set(ThreadLocal<?> key, Object value) {

            // We don't use a fast path as with get() because it is at
            // least as common to use set() to create new entries as
            // it is to replace existing ones, in which case, a fast
            // path would fail more often than not.

            Entry[] tab = table;
            int len = tab.length;
            //获取索引值，这个地方是比较特别的地方
            int i = key.threadLocalHashCode & (len-1);

            //遍历tab如果已经存在则更新值
            for (Entry e = tab[i];
                 e != null;
                 e = tab[i = nextIndex(i, len)]) {
                ThreadLocal<?> k = e.get();

                if (k == key) {
                    e.value = value;
                    return;
                }

                if (k == null) {
                    replaceStaleEntry(key, value, i);
                    return;
                }
            }
            
            //如果上面没有遍历成功则创建新值
            tab[i] = new Entry(key, value);
            int sz = ++size;
            //满足条件数组扩容x2
            if (!cleanSomeSlots(i, sz) && sz >= threshold)
                rehash();
        }

总结如下：

对于某一ThreadLocal来讲，他的索引值i是确定的，在不同线程之间访问时访问的是不同的table数组的同一位置即都为table[i]，只不过这个不同线程之间的table是独立的。
对于同一线程的不同ThreadLocal来讲，这些ThreadLocal实例共享一个table数组，然后每个ThreadLocal实例在table中的索引i是不同的。

    
    //ThreadLocal中get方法
public T get() {
    Thread t = Thread.currentThread();
    ThreadLocalMap map = getMap(t);
    if (map != null) {
        ThreadLocalMap.Entry e = map.getEntry(this);
        if (e != null) {
            @SuppressWarnings("unchecked")
            T result = (T)e.value;
            return result;
        }
    }
    return setInitialValue();
}
    
//ThreadLocalMap中getEntry方法
private Entry getEntry(ThreadLocal<?> key) {
       int i = key.threadLocalHashCode & (table.length - 1);
       Entry e = table[i];
       if (e != null && e.get() == key)
            return e;
       else
            return getEntryAfterMiss(key, i, e);
   }
理解了set方法，get方法也就清楚明了，无非是通过计算出索引直接从数组对应位置读取即可。

ThreadLocal实现主要涉及Thread，ThreadLocal，ThreadLocalMap这三个类。关于ThreadLocal的实现流程正如上面写的那样，实际代码还有许多细节处理的部分并没有在这里写出来。

ThreadLocal特性

ThreadLocal和Synchronized都是为了解决多线程中相同变量的访问冲突问题，不同的点是

Synchronized是通过线程等待，牺牲时间来解决访问冲突
ThreadLocal是通过每个线程单独一份存储空间，牺牲空间来解决冲突，并且相比于Synchronized，ThreadLocal具有线程隔离的效果，只有在线程内才能获取到对应的值，线程外则不能访问到想要的值。
正因为ThreadLocal的线程隔离特性，使他的应用场景相对来说更为特殊一些。在android中Looper、ActivityThread以及AMS中都用到了ThreadLocal。当某些数据是以线程为作用域并且不同线程具有不同的数据副本的时候，就可以考虑采用ThreadLocal。

    
 对于ThreadLocal来说，其存在静态内部类ThreadLocalMap为每个Thread都维护了一个数组table，对于同一个线程来说，其可以有多个ThreadLocal，通过其key值与table长度-1值的按位与进行位置i的计算，然后将ThreadLocal插入该table当中，如果该位置存在只更新，不存在创建。
    
对于某一ThreadLocal来讲，他的索引值i是确定的，在不同线程之间访问时访问的是不同的table数组的同一位置即都为table[i]，只不过这个不同线程之间的table是独立的。
对于同一线程的不同ThreadLocal来讲，这些ThreadLocal实例共享一个table数组，然后每个ThreadLocal实例在table中的索引i是不同的。
 
    
```

## 51，hash表存储效率只有50%的原因

```
Hash Table 常用于频繁进行 key/value 模式的查找中。(查找模式，如匹配查找)
哈希表最大的优点在于查找速度快，但存储时可能发生collision(冲突)。
哈希表大多使用open addressing来解决collision，此时search的时间复杂度计算公式为：1/( 1 - n/m )
其中，n与m分别表示存储的记录数与哈希表的长度，即装填因子( [load factor ](http://en.wikipedia.org/wiki/Load_factor_(computer_science)#Key_statistics))
 故，若哈希表半满，即 n/m >= 1/2，则每次的search次数可能会 >= 2 
```

## 52.反射

## 53.抽象类 接口

## 54.futureTask

## 55.过滤器

## 56.*Optional*  